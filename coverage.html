
<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<title>config: Go Coverage Report</title>
		<style>
			body {
				background: black;
				color: rgb(80, 80, 80);
			}
			body, pre, #legend span {
				font-family: Menlo, monospace;
				font-weight: bold;
			}
			#topbar {
				background: black;
				position: fixed;
				top: 0; left: 0; right: 0;
				height: 42px;
				border-bottom: 1px solid rgb(80, 80, 80);
			}
			#content {
				margin-top: 50px;
			}
			#nav, #legend {
				float: left;
				margin-left: 10px;
			}
			#legend {
				margin-top: 12px;
			}
			#nav {
				margin-top: 10px;
			}
			#legend span {
				margin: 0 5px;
			}
			.cov0 { color: rgb(192, 0, 0) }
.cov1 { color: rgb(128, 128, 128) }
.cov2 { color: rgb(116, 140, 131) }
.cov3 { color: rgb(104, 152, 134) }
.cov4 { color: rgb(92, 164, 137) }
.cov5 { color: rgb(80, 176, 140) }
.cov6 { color: rgb(68, 188, 143) }
.cov7 { color: rgb(56, 200, 146) }
.cov8 { color: rgb(44, 212, 149) }
.cov9 { color: rgb(32, 224, 152) }
.cov10 { color: rgb(20, 236, 155) }

		</style>
	</head>
	<body>
		<div id="topbar">
			<div id="nav">
				<select id="files">
				
				<option value="file0">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config/config.go (53.3%)</option>
				
				<option value="file1">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/helm/helm.go (0.0%)</option>
				
				<option value="file2">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/http/authorization.go (98.6%)</option>
				
				<option value="file3">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/http/http.go (79.4%)</option>
				
				<option value="file4">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/http/middleware.go (86.4%)</option>
				
				<option value="file5">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/http/sts.go (100.0%)</option>
				
				<option value="file6">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/http/wellknown.go (81.1%)</option>
				
				<option value="file7">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes-mcp-server/cmd/root.go (68.9%)</option>
				
				<option value="file8">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/accesscontrol.go (0.0%)</option>
				
				<option value="file9">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/accesscontrol_clientset.go (11.5%)</option>
				
				<option value="file10">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/accesscontrol_restmapper.go (3.6%)</option>
				
				<option value="file11">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/configuration.go (33.3%)</option>
				
				<option value="file12">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/events.go (0.0%)</option>
				
				<option value="file13">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/health.go (95.5%)</option>
				
				<option value="file14">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/impersonate_roundtripper.go (0.0%)</option>
				
				<option value="file15">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/kubernetes.go (45.2%)</option>
				
				<option value="file16">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/multicluster.go (60.3%)</option>
				
				<option value="file17">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/namespaces.go (0.0%)</option>
				
				<option value="file18">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/nsk_integration.go (52.8%)</option>
				
				<option value="file19">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/openshift.go (0.0%)</option>
				
				<option value="file20">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/pods.go (0.0%)</option>
				
				<option value="file21">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/resources.go (0.0%)</option>
				
				<option value="file22">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/kubernetes/token.go (0.0%)</option>
				
				<option value="file23">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/nsk/client.go (4.7%)</option>
				
				<option value="file24">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/nsk/manager.go (0.0%)</option>
				
				<option value="file25">github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/output/output.go (30.8%)</option>
				
				</select>
			</div>
			<div id="legend">
				<span>not tracked</span>
			
				<span class="cov0">no coverage</span>
				<span class="cov1">low coverage</span>
				<span class="cov2">*</span>
				<span class="cov3">*</span>
				<span class="cov4">*</span>
				<span class="cov5">*</span>
				<span class="cov6">*</span>
				<span class="cov7">*</span>
				<span class="cov8">*</span>
				<span class="cov9">*</span>
				<span class="cov10">high coverage</span>
			
			</div>
		</div>
		<div id="content">
		
		<pre class="file" id="file0" style="display: none">package config

import (
        "fmt"
        "os"

        "github.com/BurntSushi/toml"
)

// StaticConfig is the configuration for the server.
// It allows to configure server specific settings and tools to be enabled or disabled.
type StaticConfig struct {
        DeniedResources []GroupVersionKind `toml:"denied_resources"`

        LogLevel   int    `toml:"log_level,omitempty"`
        Port       string `toml:"port,omitempty"`
        SSEBaseURL string `toml:"sse_base_url,omitempty"`
        // Legacy single kubeconfig support (deprecated in favor of multi-cluster)
        KubeConfig string `toml:"kubeconfig,omitempty"`

        // Multi-cluster configuration
        KubeConfigDir  string            `toml:"kubeconfig_dir,omitempty"`
        DefaultCluster string            `toml:"default_cluster,omitempty"`
        AutoDiscovery  bool              `toml:"auto_discovery,omitempty"`
        ClusterAliases map[string]string `toml:"cluster_aliases,omitempty"`

        // NSK Integration
        NSKIntegration *NSKConfig `toml:"nsk,omitempty"`

        ListOutput string `toml:"list_output,omitempty"`
        // When true, expose only tools annotated with readOnlyHint=true
        ReadOnly bool `toml:"read_only,omitempty"`
        // When true, disable tools annotated with destructiveHint=true
        DisableDestructive bool     `toml:"disable_destructive,omitempty"`
        EnabledTools       []string `toml:"enabled_tools,omitempty"`
        DisabledTools      []string `toml:"disabled_tools,omitempty"`

        // Authorization-related fields
        // RequireOAuth indicates whether the server requires OAuth for authentication.
        RequireOAuth bool `toml:"require_oauth,omitempty"`
        // OAuthAudience is the valid audience for the OAuth tokens, used for offline JWT claim validation.
        OAuthAudience string `toml:"oauth_audience,omitempty"`
        // ValidateToken indicates whether the server should validate the token against the Kubernetes API Server using TokenReview.
        ValidateToken bool `toml:"validate_token,omitempty"`
        // AuthorizationURL is the URL of the OIDC authorization server.
        // It is used for token validation and for STS token exchange.
        AuthorizationURL string `toml:"authorization_url,omitempty"`
        // DisableDynamicClientRegistration indicates whether dynamic client registration is disabled.
        // If true, the .well-known endpoints will not expose the registration endpoint.
        DisableDynamicClientRegistration bool `toml:"disable_dynamic_client_registration,omitempty"`
        // OAuthScopes are the supported **client** scopes requested during the **client/frontend** OAuth flow.
        OAuthScopes []string `toml:"oauth_scopes,omitempty"`
        // StsClientId is the OAuth client ID used for backend token exchange
        StsClientId string `toml:"sts_client_id,omitempty"`
        // StsClientSecret is the OAuth client secret used for backend token exchange
        StsClientSecret string `toml:"sts_client_secret,omitempty"`
        // StsAudience is the audience for the STS token exchange.
        StsAudience string `toml:"sts_audience,omitempty"`
        // StsScopes is the scopes for the STS token exchange.
        StsScopes            []string `toml:"sts_scopes,omitempty"`
        CertificateAuthority string   `toml:"certificate_authority,omitempty"`
        ServerURL            string   `toml:"server_url,omitempty"`
}

type GroupVersionKind struct {
        Group   string `toml:"group"`
        Version string `toml:"version"`
        Kind    string `toml:"kind,omitempty"`
}

// NSKConfig represents configuration for NSK (Netskope Kubernetes) tool integration
type NSKConfig struct {
        // Core NSK settings
        Enabled bool   `toml:"enabled,omitempty"`
        NSKPath string `toml:"nsk_path,omitempty"` // Path to NSK binary, defaults to "nsk"

        // Rancher environment settings
        RancherURL   string `toml:"rancher_url,omitempty"`
        RancherToken string `toml:"rancher_token,omitempty"`
        Profile      string `toml:"profile,omitempty"`
        ConfigDir    string `toml:"config_dir,omitempty"`

        // Auto-refresh settings
        AutoRefresh     bool   `toml:"auto_refresh,omitempty"`
        RefreshInterval string `toml:"refresh_interval,omitempty"` // e.g., "1h", "30m"

        // Cluster filtering
        ClusterPattern  string   `toml:"cluster_pattern,omitempty"` // Regex pattern for cluster names
        ExcludeClusters []string `toml:"exclude_clusters,omitempty"`
        IncludeClusters []string `toml:"include_clusters,omitempty"`

        // Environment variables for NSK execution
        Environment map[string]string `toml:"environment,omitempty"`
}

// ReadConfig reads the toml file and returns the StaticConfig.
func ReadConfig(configPath string) (*StaticConfig, error) <span class="cov10" title="3">{
        configData, err := os.ReadFile(configPath)
        if err != nil </span><span class="cov1" title="1">{
                return nil, err
        }</span>

        <span class="cov6" title="2">var config *StaticConfig
        err = toml.Unmarshal(configData, &amp;config)
        if err != nil </span><span class="cov1" title="1">{
                return nil, err
        }</span>
        <span class="cov1" title="1">return config, nil</span>
}

// ValidateMultiClusterConfig validates the multi-cluster configuration
func (c *StaticConfig) ValidateMultiClusterConfig() error <span class="cov0" title="0">{
        if c.KubeConfigDir == "" &amp;&amp; c.KubeConfig == "" </span><span class="cov0" title="0">{
                return fmt.Errorf("either kubeconfig_dir or kubeconfig must be specified")
        }</span>

        <span class="cov0" title="0">if c.KubeConfigDir != "" &amp;&amp; c.KubeConfig != "" </span><span class="cov0" title="0">{
                return fmt.Errorf("cannot specify both kubeconfig_dir and kubeconfig")
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// IsMultiClusterEnabled returns true if multi-cluster mode is enabled
func (c *StaticConfig) IsMultiClusterEnabled() bool <span class="cov0" title="0">{
        return c.KubeConfigDir != ""
}</span>

// IsNSKEnabled returns true if NSK integration is enabled
func (c *StaticConfig) IsNSKEnabled() bool <span class="cov0" title="0">{
        return c.NSKIntegration != nil &amp;&amp; c.NSKIntegration.Enabled
}</span>
</pre>
		
		<pre class="file" id="file1" style="display: none">package helm

import (
        "context"
        "fmt"
        "helm.sh/helm/v3/pkg/action"
        "helm.sh/helm/v3/pkg/chart/loader"
        "helm.sh/helm/v3/pkg/cli"
        "helm.sh/helm/v3/pkg/registry"
        "helm.sh/helm/v3/pkg/release"
        "k8s.io/cli-runtime/pkg/genericclioptions"
        "log"
        "sigs.k8s.io/yaml"
        "time"
)

type Kubernetes interface {
        genericclioptions.RESTClientGetter
        NamespaceOrDefault(namespace string) string
}

type Helm struct {
        kubernetes Kubernetes
}

// NewHelm creates a new Helm instance
func NewHelm(kubernetes Kubernetes) *Helm <span class="cov0" title="0">{
        return &amp;Helm{kubernetes: kubernetes}
}</span>

func (h *Helm) Install(ctx context.Context, chart string, values map[string]interface{}, name string, namespace string) (string, error) <span class="cov0" title="0">{
        cfg, err := h.newAction(h.kubernetes.NamespaceOrDefault(namespace), false)
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">install := action.NewInstall(cfg)
        if name == "" </span><span class="cov0" title="0">{
                install.GenerateName = true
                install.ReleaseName, _, _ = install.NameAndChart([]string{chart})
        }</span> else<span class="cov0" title="0"> {
                install.ReleaseName = name
        }</span>
        <span class="cov0" title="0">install.Namespace = h.kubernetes.NamespaceOrDefault(namespace)
        install.Wait = true
        install.Timeout = 5 * time.Minute
        install.DryRun = false

        chartRequested, err := install.LocateChart(chart, cli.New())
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">chartLoaded, err := loader.Load(chartRequested)
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>

        <span class="cov0" title="0">installedRelease, err := install.RunWithContext(ctx, chartLoaded, values)
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">ret, err := yaml.Marshal(simplify(installedRelease))
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">return string(ret), nil</span>
}

// List lists all the releases for the specified namespace (or current namespace if). Or allNamespaces is true, it lists all releases across all namespaces.
func (h *Helm) List(namespace string, allNamespaces bool) (string, error) <span class="cov0" title="0">{
        cfg, err := h.newAction(namespace, allNamespaces)
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">list := action.NewList(cfg)
        list.AllNamespaces = allNamespaces
        releases, err := list.Run()
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span> else<span class="cov0" title="0"> if len(releases) == 0 </span><span class="cov0" title="0">{
                return "No Helm releases found", nil
        }</span>
        <span class="cov0" title="0">ret, err := yaml.Marshal(simplify(releases...))
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">return string(ret), nil</span>
}

func (h *Helm) Uninstall(name string, namespace string) (string, error) <span class="cov0" title="0">{
        cfg, err := h.newAction(h.kubernetes.NamespaceOrDefault(namespace), false)
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">uninstall := action.NewUninstall(cfg)
        uninstall.IgnoreNotFound = true
        uninstall.Wait = true
        uninstall.Timeout = 5 * time.Minute
        uninstalledRelease, err := uninstall.Run(name)
        if uninstalledRelease == nil &amp;&amp; err == nil </span><span class="cov0" title="0">{
                return fmt.Sprintf("Release %s not found", name), nil
        }</span> else<span class="cov0" title="0"> if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">return fmt.Sprintf("Uninstalled release %s %s", uninstalledRelease.Release.Name, uninstalledRelease.Info), nil</span>
}

func (h *Helm) newAction(namespace string, allNamespaces bool) (*action.Configuration, error) <span class="cov0" title="0">{
        cfg := new(action.Configuration)
        applicableNamespace := ""
        if !allNamespaces </span><span class="cov0" title="0">{
                applicableNamespace = h.kubernetes.NamespaceOrDefault(namespace)
        }</span>
        <span class="cov0" title="0">registryClient, err := registry.NewClient()
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov0" title="0">cfg.RegistryClient = registryClient
        return cfg, cfg.Init(h.kubernetes, applicableNamespace, "", log.Printf)</span>
}

func simplify(release ...*release.Release) []map[string]interface{} <span class="cov0" title="0">{
        ret := make([]map[string]interface{}, len(release))
        for i, r := range release </span><span class="cov0" title="0">{
                ret[i] = map[string]interface{}{
                        "name":      r.Name,
                        "namespace": r.Namespace,
                        "revision":  r.Version,
                }
                if r.Chart != nil </span><span class="cov0" title="0">{
                        ret[i]["chart"] = r.Chart.Metadata.Name
                        ret[i]["chartVersion"] = r.Chart.Metadata.Version
                        ret[i]["appVersion"] = r.Chart.Metadata.AppVersion
                }</span>
                <span class="cov0" title="0">if r.Info != nil </span><span class="cov0" title="0">{
                        ret[i]["status"] = r.Info.Status.String()
                        if !r.Info.LastDeployed.IsZero() </span><span class="cov0" title="0">{
                                ret[i]["lastDeployed"] = r.Info.LastDeployed.Format(time.RFC1123Z)
                        }</span>
                }
        }
        <span class="cov0" title="0">return ret</span>
}
</pre>
		
		<pre class="file" id="file2" style="display: none">package http

import (
        "context"
        "fmt"
        "net/http"
        "strings"

        "github.com/coreos/go-oidc/v3/oidc"
        "github.com/go-jose/go-jose/v4"
        "github.com/go-jose/go-jose/v4/jwt"
        "golang.org/x/oauth2"
        authenticationapiv1 "k8s.io/api/authentication/v1"
        "k8s.io/klog/v2"
        "k8s.io/utils/strings/slices"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config"
        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/mcp"
)

type KubernetesApiTokenVerifier interface {
        // KubernetesApiVerifyToken TODO: clarify proper implementation
        KubernetesApiVerifyToken(ctx context.Context, token, audience string) (*authenticationapiv1.UserInfo, []string, error)
}

// AuthorizationMiddleware validates the OAuth flow for protected resources.
//
// The flow is skipped for unprotected resources, such as health checks and well-known endpoints.
//
//        There are several auth scenarios supported by this middleware:
//
//         1. requireOAuth is false:
//
//            - The OAuth flow is skipped, and the server is effectively unprotected.
//            - The request is passed to the next handler without any validation.
//
//            see TestAuthorizationRequireOAuthFalse
//
//         2. requireOAuth is set to true, server is protected:
//
//            2.1. Raw Token Validation (oidcProvider is nil):
//                 - The token is validated offline for basic sanity checks (expiration).
//                 - If OAuthAudience is set, the token is validated against the audience.
//                 - If ValidateToken is set, the token is then used against the Kubernetes API Server for TokenReview.
//
//                 see TestAuthorizationRawToken
//
//            2.2. OIDC Provider Validation (oidcProvider is not nil):
//                 - The token is validated offline for basic sanity checks (audience and expiration).
//                 - If OAuthAudience is set, the token is validated against the audience.
//                 - The token is then validated against the OIDC Provider.
//                 - If ValidateToken is set, the token is then used against the Kubernetes API Server for TokenReview.
//
//                 see TestAuthorizationOidcToken
//
//            2.3. OIDC Token Exchange (oidcProvider is not nil, StsClientId and StsAudience are set):
//                 - The token is validated offline for basic sanity checks (audience and expiration).
//                 - If OAuthAudience is set, the token is validated against the audience.
//                 - The token is then validated against the OIDC Provider.
//                 - If the token is valid, an external account token exchange is performed using
//                   the OIDC Provider to obtain a new token with the specified audience and scopes.
//                 - If ValidateToken is set, the exchanged token is then used against the Kubernetes API Server for TokenReview.
//
//                 see TestAuthorizationOidcTokenExchange
func AuthorizationMiddleware(staticConfig *config.StaticConfig, oidcProvider *oidc.Provider, verifier KubernetesApiTokenVerifier) func(http.Handler) http.Handler <span class="cov9" title="27">{
        return func(next http.Handler) http.Handler </span><span class="cov9" title="27">{
                return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) </span><span class="cov10" title="38">{
                        if r.URL.Path == healthEndpoint || slices.Contains(WellKnownEndpoints, r.URL.EscapedPath()) </span><span class="cov8" title="18">{
                                next.ServeHTTP(w, r)
                                return
                        }</span>
                        <span class="cov8" title="20">if !staticConfig.RequireOAuth </span><span class="cov4" title="5">{
                                next.ServeHTTP(w, r)
                                return
                        }</span>

                        <span class="cov7" title="15">wwwAuthenticateHeader := "Bearer realm=\"Kubernetes MCP Server\""
                        if staticConfig.OAuthAudience != "" </span><span class="cov6" title="9">{
                                wwwAuthenticateHeader += fmt.Sprintf(`, audience="%s"`, staticConfig.OAuthAudience)
                        }</span>

                        <span class="cov7" title="15">authHeader := r.Header.Get("Authorization")
                        if authHeader == "" || !strings.HasPrefix(authHeader, "Bearer ") </span><span class="cov2" title="2">{
                                klog.V(1).Infof("Authentication failed - missing or invalid bearer token: %s %s from %s", r.Method, r.URL.Path, r.RemoteAddr)

                                w.Header().Set("WWW-Authenticate", wwwAuthenticateHeader+", error=\"missing_token\"")
                                http.Error(w, "Unauthorized: Bearer token required", http.StatusUnauthorized)
                                return
                        }</span>

                        <span class="cov7" title="13">token := strings.TrimPrefix(authHeader, "Bearer ")

                        claims, err := ParseJWTClaims(token)
                        if err == nil &amp;&amp; claims == nil </span><span class="cov0" title="0">{
                                // Impossible case, but just in case
                                err = fmt.Errorf("failed to parse JWT claims from token")
                        }</span>
                        // Offline validation
                        <span class="cov7" title="13">if err == nil </span><span class="cov7" title="12">{
                                err = claims.ValidateOffline(staticConfig.OAuthAudience)
                        }</span>
                        // Online OIDC provider validation
                        <span class="cov7" title="13">if err == nil </span><span class="cov6" title="10">{
                                err = claims.ValidateWithProvider(r.Context(), staticConfig.OAuthAudience, oidcProvider)
                        }</span>
                        // Scopes propagation, they are likely to be used for authorization.
                        <span class="cov7" title="13">if err == nil </span><span class="cov6" title="9">{
                                scopes := claims.GetScopes()
                                klog.V(2).Infof("JWT token validated - Scopes: %v", scopes)
                                r = r.WithContext(context.WithValue(r.Context(), mcp.TokenScopesContextKey, scopes))
                        }</span>
                        // Token exchange with OIDC provider
                        <span class="cov7" title="13">sts := NewFromConfig(staticConfig, oidcProvider)
                        // TODO: Maybe the token had already been exchanged, if it has the right audience and scopes, we can skip this step.
                        if err == nil &amp;&amp; sts.IsEnabled() </span><span class="cov2" title="2">{
                                var exchangedToken *oauth2.Token
                                // If the token is valid, we can exchange it for a new token with the specified audience and scopes.
                                exchangedToken, err = sts.ExternalAccountTokenExchange(r.Context(), &amp;oauth2.Token{
                                        AccessToken: claims.Token,
                                        TokenType:   "Bearer",
                                })
                                if err == nil </span><span class="cov2" title="2">{
                                        // Replace the original token with the exchanged token
                                        token = exchangedToken.AccessToken
                                        claims, err = ParseJWTClaims(token)
                                        r.Header.Set("Authorization", fmt.Sprintf("Bearer %s", token)) // TODO: Implement test to verify, THIS IS A CRITICAL PART
                                }</span>
                        }
                        // Kubernetes API Server TokenReview validation
                        <span class="cov7" title="13">if err == nil &amp;&amp; staticConfig.ValidateToken </span><span class="cov4" title="5">{
                                err = claims.ValidateWithKubernetesApi(r.Context(), staticConfig.OAuthAudience, verifier)
                        }</span>
                        <span class="cov7" title="13">if err != nil </span><span class="cov4" title="5">{
                                klog.V(1).Infof("Authentication failed - JWT validation error: %s %s from %s, error: %v", r.Method, r.URL.Path, r.RemoteAddr, err)

                                w.Header().Set("WWW-Authenticate", wwwAuthenticateHeader+", error=\"invalid_token\"")
                                http.Error(w, "Unauthorized: Invalid token", http.StatusUnauthorized)
                                return
                        }</span>

                        <span class="cov6" title="8">next.ServeHTTP(w, r)</span>
                })
        }
}

var allSignatureAlgorithms = []jose.SignatureAlgorithm{
        jose.EdDSA,
        jose.HS256,
        jose.HS384,
        jose.HS512,
        jose.RS256,
        jose.RS384,
        jose.RS512,
        jose.ES256,
        jose.ES384,
        jose.ES512,
        jose.PS256,
        jose.PS384,
        jose.PS512,
}

type JWTClaims struct {
        jwt.Claims
        Token string `json:"-"`
        Scope string `json:"scope,omitempty"`
}

func (c *JWTClaims) GetScopes() []string <span class="cov7" title="14">{
        if c.Scope == "" </span><span class="cov6" title="10">{
                return nil
        }</span>
        <span class="cov4" title="4">return strings.Fields(c.Scope)</span>
}

// ValidateOffline Checks if the JWT claims are valid and if the audience matches the expected one.
func (c *JWTClaims) ValidateOffline(audience string) error <span class="cov7" title="15">{
        expected := jwt.Expected{}
        if audience != "" </span><span class="cov7" title="12">{
                expected.AnyAudience = jwt.Audience{audience}
        }</span>
        <span class="cov7" title="15">if err := c.Validate(expected); err != nil </span><span class="cov4" title="4">{
                return fmt.Errorf("JWT token validation error: %v", err)
        }</span>
        <span class="cov6" title="11">return nil</span>
}

// ValidateWithProvider validates the JWT claims against the OIDC provider.
func (c *JWTClaims) ValidateWithProvider(ctx context.Context, audience string, provider *oidc.Provider) error <span class="cov6" title="10">{
        if provider != nil </span><span class="cov5" title="6">{
                verifier := provider.Verifier(&amp;oidc.Config{
                        ClientID: audience,
                })
                _, err := verifier.Verify(ctx, c.Token)
                if err != nil </span><span class="cov1" title="1">{
                        return fmt.Errorf("OIDC token validation error: %v", err)
                }</span>
        }
        <span class="cov6" title="9">return nil</span>
}

func (c *JWTClaims) ValidateWithKubernetesApi(ctx context.Context, audience string, verifier KubernetesApiTokenVerifier) error <span class="cov4" title="5">{
        if verifier != nil </span><span class="cov4" title="5">{
                _, _, err := verifier.KubernetesApiVerifyToken(ctx, c.Token, audience)
                if err != nil </span><span class="cov1" title="1">{
                        return fmt.Errorf("kubernetes API token validation error: %v", err)
                }</span>
        }
        <span class="cov4" title="4">return nil</span>
}

func ParseJWTClaims(token string) (*JWTClaims, error) <span class="cov8" title="24">{
        tkn, err := jwt.ParseSigned(token, allSignatureAlgorithms)
        if err != nil </span><span class="cov3" title="3">{
                return nil, fmt.Errorf("failed to parse JWT token: %w", err)
        }</span>
        <span class="cov8" title="21">claims := &amp;JWTClaims{}
        err = tkn.UnsafeClaimsWithoutVerification(claims)
        claims.Token = token
        return claims, err</span>
}
</pre>
		
		<pre class="file" id="file3" style="display: none">package http

import (
        "context"
        "errors"
        "net/http"
        "os"
        "os/signal"
        "syscall"
        "time"

        "github.com/coreos/go-oidc/v3/oidc"

        "k8s.io/klog/v2"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config"
        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/mcp"
)

const (
        healthEndpoint     = "/healthz"
        mcpEndpoint        = "/mcp"
        sseEndpoint        = "/sse"
        sseMessageEndpoint = "/message"
)

func Serve(ctx context.Context, mcpServer *mcp.Server, staticConfig *config.StaticConfig, oidcProvider *oidc.Provider) error <span class="cov10" title="27">{
        mux := http.NewServeMux()

        wrappedMux := RequestMiddleware(
                AuthorizationMiddleware(staticConfig, oidcProvider, mcpServer)(mux),
        )

        httpServer := &amp;http.Server{
                Addr:    ":" + staticConfig.Port,
                Handler: wrappedMux,
        }

        sseServer := mcpServer.ServeSse(staticConfig.SSEBaseURL, httpServer)
        streamableHttpServer := mcpServer.ServeHTTP(httpServer)
        mux.Handle(sseEndpoint, sseServer)
        mux.Handle(sseMessageEndpoint, sseServer)
        mux.Handle(mcpEndpoint, streamableHttpServer)
        mux.HandleFunc(healthEndpoint, func(w http.ResponseWriter, r *http.Request) </span><span class="cov2" title="2">{
                w.WriteHeader(http.StatusOK)
        }</span>)
        <span class="cov10" title="27">mux.Handle("/.well-known/", WellKnownHandler(staticConfig))

        ctx, cancel := context.WithCancel(ctx)
        defer cancel()

        sigChan := make(chan os.Signal, 1)
        signal.Notify(sigChan, syscall.SIGINT, syscall.SIGHUP, syscall.SIGTERM)

        serverErr := make(chan error, 1)
        go func() </span><span class="cov10" title="27">{
                klog.V(0).Infof("Streaming and SSE HTTP servers starting on port %s and paths /mcp, /sse, /message", staticConfig.Port)
                if err := httpServer.ListenAndServe(); err != nil &amp;&amp; !errors.Is(err, http.ErrServerClosed) </span><span class="cov0" title="0">{
                        serverErr &lt;- err
                }</span>
        }()

        <span class="cov10" title="27">select </span>{
        case sig := &lt;-sigChan:<span class="cov0" title="0">
                klog.V(0).Infof("Received signal %v, initiating graceful shutdown", sig)
                cancel()</span>
        case &lt;-ctx.Done():<span class="cov10" title="27">
                klog.V(0).Infof("Context cancelled, initiating graceful shutdown")</span>
        case err := &lt;-serverErr:<span class="cov0" title="0">
                klog.Errorf("HTTP server error: %v", err)
                return err</span>
        }

        <span class="cov10" title="27">shutdownCtx, shutdownCancel := context.WithTimeout(context.Background(), 10*time.Second)
        defer shutdownCancel()

        klog.V(0).Infof("Shutting down HTTP server gracefully...")
        if err := httpServer.Shutdown(shutdownCtx); err != nil </span><span class="cov0" title="0">{
                klog.Errorf("HTTP server shutdown error: %v", err)
                return err
        }</span>

        <span class="cov10" title="27">klog.V(0).Infof("HTTP server shutdown complete")
        return nil</span>
}
</pre>
		
		<pre class="file" id="file4" style="display: none">package http

import (
        "bufio"
        "net"
        "net/http"
        "time"

        "k8s.io/klog/v2"
)

func RequestMiddleware(next http.Handler) http.Handler <span class="cov9" title="27">{
        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) </span><span class="cov10" title="38">{
                if r.URL.Path == "/healthz" </span><span class="cov2" title="2">{
                        next.ServeHTTP(w, r)
                        return
                }</span>

                <span class="cov9" title="36">start := time.Now()

                lrw := &amp;loggingResponseWriter{
                        ResponseWriter: w,
                        statusCode:     http.StatusOK,
                }

                next.ServeHTTP(lrw, r)

                duration := time.Since(start)
                klog.V(5).Infof("%s %s %d %v", r.Method, r.URL.Path, lrw.statusCode, duration)</span>
        })
}

type loggingResponseWriter struct {
        http.ResponseWriter
        statusCode    int
        headerWritten bool
}

func (lrw *loggingResponseWriter) WriteHeader(code int) <span class="cov9" title="35">{
        if !lrw.headerWritten </span><span class="cov9" title="35">{
                lrw.statusCode = code
                lrw.headerWritten = true
                lrw.ResponseWriter.WriteHeader(code)
        }</span>
}

func (lrw *loggingResponseWriter) Write(b []byte) (int, error) <span class="cov9" title="26">{
        if !lrw.headerWritten </span><span class="cov1" title="1">{
                lrw.statusCode = http.StatusOK
                lrw.headerWritten = true
        }</span>
        <span class="cov9" title="26">return lrw.ResponseWriter.Write(b)</span>
}

func (lrw *loggingResponseWriter) Flush() <span class="cov7" title="12">{
        if flusher, ok := lrw.ResponseWriter.(http.Flusher); ok </span><span class="cov7" title="12">{
                flusher.Flush()
        }</span>
}

func (lrw *loggingResponseWriter) Hijack() (net.Conn, *bufio.ReadWriter, error) <span class="cov0" title="0">{
        if hijacker, ok := lrw.ResponseWriter.(http.Hijacker); ok </span><span class="cov0" title="0">{
                return hijacker.Hijack()
        }</span>
        <span class="cov0" title="0">return nil, nil, http.ErrNotSupported</span>
}
</pre>
		
		<pre class="file" id="file5" style="display: none">package http

import (
        "context"

        "github.com/coreos/go-oidc/v3/oidc"
        "golang.org/x/oauth2"
        "golang.org/x/oauth2/google/externalaccount"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config"
)

type staticSubjectTokenSupplier struct {
        token string
}

func (s *staticSubjectTokenSupplier) SubjectToken(_ context.Context, _ externalaccount.SupplierOptions) (string, error) <span class="cov5" title="4">{
        return s.token, nil
}</span>

var _ externalaccount.SubjectTokenSupplier = &amp;staticSubjectTokenSupplier{}

type SecurityTokenService struct {
        *oidc.Provider
        ClientId                string
        ClientSecret            string
        ExternalAccountAudience string
        ExternalAccountScopes   []string
}

func NewFromConfig(config *config.StaticConfig, provider *oidc.Provider) *SecurityTokenService <span class="cov8" title="13">{
        return &amp;SecurityTokenService{
                Provider:                provider,
                ClientId:                config.StsClientId,
                ClientSecret:            config.StsClientSecret,
                ExternalAccountAudience: config.StsAudience,
                ExternalAccountScopes:   config.StsScopes,
        }
}</span>

func (sts *SecurityTokenService) IsEnabled() bool <span class="cov10" title="18">{
        return sts.Provider != nil &amp;&amp; sts.ClientId != "" &amp;&amp; sts.ExternalAccountAudience != ""
}</span>

func (sts *SecurityTokenService) ExternalAccountTokenExchange(ctx context.Context, originalToken *oauth2.Token) (*oauth2.Token, error) <span class="cov6" title="5">{
        ts, err := externalaccount.NewTokenSource(ctx, externalaccount.Config{
                TokenURL:             sts.Endpoint().TokenURL,
                ClientID:             sts.ClientId,
                ClientSecret:         sts.ClientSecret,
                Audience:             sts.ExternalAccountAudience,
                SubjectTokenType:     "urn:ietf:params:oauth:token-type:access_token",
                SubjectTokenSupplier: &amp;staticSubjectTokenSupplier{token: originalToken.AccessToken},
                Scopes:               sts.ExternalAccountScopes,
        })
        if err != nil </span><span class="cov1" title="1">{
                return nil, err
        }</span>
        <span class="cov5" title="4">return ts.Token()</span>
}
</pre>
		
		<pre class="file" id="file6" style="display: none">package http

import (
        "encoding/json"
        "fmt"
        "net/http"
        "strings"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config"
)

const (
        oauthAuthorizationServerEndpoint = "/.well-known/oauth-authorization-server"
        oauthProtectedResourceEndpoint   = "/.well-known/oauth-protected-resource"
        openIDConfigurationEndpoint      = "/.well-known/openid-configuration"
)

var WellKnownEndpoints = []string{
        oauthAuthorizationServerEndpoint,
        oauthProtectedResourceEndpoint,
        openIDConfigurationEndpoint,
}

type WellKnown struct {
        authorizationUrl                 string
        scopesSupported                  []string
        disableDynamicClientRegistration bool
}

var _ http.Handler = &amp;WellKnown{}

func WellKnownHandler(staticConfig *config.StaticConfig) http.Handler <span class="cov10" title="27">{
        authorizationUrl := staticConfig.AuthorizationURL
        if authorizationUrl != "" &amp;&amp; strings.HasSuffix("authorizationUrl", "/") </span><span class="cov0" title="0">{
                authorizationUrl = strings.TrimSuffix(authorizationUrl, "/")
        }</span>
        <span class="cov10" title="27">return &amp;WellKnown{
                authorizationUrl:                 authorizationUrl,
                disableDynamicClientRegistration: staticConfig.DisableDynamicClientRegistration,
                scopesSupported:                  staticConfig.OAuthScopes,
        }</span>
}

func (w WellKnown) ServeHTTP(writer http.ResponseWriter, request *http.Request) <span class="cov8" title="16">{
        if w.authorizationUrl == "" </span><span class="cov4" title="4">{
                http.Error(writer, "Authorization URL is not configured", http.StatusNotFound)
                return
        }</span>
        <span class="cov7" title="12">req, err := http.NewRequest(request.Method, w.authorizationUrl+request.URL.EscapedPath(), nil)
        if err != nil </span><span class="cov0" title="0">{
                http.Error(writer, "Failed to create request: "+err.Error(), http.StatusInternalServerError)
                return
        }</span>
        <span class="cov7" title="12">resp, err := http.DefaultClient.Do(req.WithContext(request.Context()))
        if err != nil </span><span class="cov0" title="0">{
                http.Error(writer, "Failed to perform request: "+err.Error(), http.StatusInternalServerError)
                return
        }</span>
        <span class="cov7" title="12">defer func() </span><span class="cov7" title="12">{ _ = resp.Body.Close() }</span>()
        <span class="cov7" title="12">var resourceMetadata map[string]interface{}
        err = json.NewDecoder(resp.Body).Decode(&amp;resourceMetadata)
        if err != nil </span><span class="cov4" title="3">{
                http.Error(writer, "Failed to read response body: "+err.Error(), http.StatusInternalServerError)
                return
        }</span>
        <span class="cov7" title="9">if w.disableDynamicClientRegistration </span><span class="cov4" title="3">{
                delete(resourceMetadata, "registration_endpoint")
                resourceMetadata["require_request_uri_registration"] = false
        }</span>
        <span class="cov7" title="9">if len(w.scopesSupported) &gt; 0 </span><span class="cov4" title="3">{
                resourceMetadata["scopes_supported"] = w.scopesSupported
        }</span>
        <span class="cov7" title="9">body, err := json.Marshal(resourceMetadata)
        if err != nil </span><span class="cov0" title="0">{
                http.Error(writer, "Failed to marshal response body: "+err.Error(), http.StatusInternalServerError)
                return
        }</span>
        <span class="cov7" title="9">for key, values := range resp.Header </span><span class="cov10" title="27">{
                for _, value := range values </span><span class="cov10" title="27">{
                        writer.Header().Add(key, value)
                }</span>
        }
        <span class="cov7" title="9">writer.Header().Set("Content-Length", fmt.Sprintf("%d", len(body)))
        writer.WriteHeader(resp.StatusCode)
        _, _ = writer.Write(body)</span>
}
</pre>
		
		<pre class="file" id="file7" style="display: none">package cmd

import (
        "context"
        "crypto/tls"
        "crypto/x509"
        "errors"
        "flag"
        "fmt"
        "net/http"
        "net/url"
        "os"
        "strconv"
        "strings"

        "github.com/coreos/go-oidc/v3/oidc"
        "github.com/spf13/cobra"

        "k8s.io/cli-runtime/pkg/genericiooptions"
        "k8s.io/klog/v2"
        "k8s.io/klog/v2/textlogger"
        "k8s.io/kubectl/pkg/util/i18n"
        "k8s.io/kubectl/pkg/util/templates"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config"
        internalhttp "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/http"
        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/mcp"
        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/output"
        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/version"
)

var (
        long     = templates.LongDesc(i18n.T("Kubernetes Model Context Protocol (MCP) server"))
        examples = templates.Examples(i18n.T(`
# show this help
kubernetes-mcp-server -h

# shows version information
kubernetes-mcp-server --version

# start STDIO server
kubernetes-mcp-server

# start a SSE server on port 8080
kubernetes-mcp-server --port 8080

# start a SSE server on port 8443 with a public HTTPS host of example.com
kubernetes-mcp-server --port 8443 --sse-base-url https://example.com:8443
`))
)

type MCPServerOptions struct {
        Version              bool
        LogLevel             int
        Port                 string
        SSEPort              int
        HttpPort             int
        SSEBaseUrl           string
        Kubeconfig           string
        KubeconfigDir        string
        Profile              string
        ListOutput           string
        ReadOnly             bool
        DisableDestructive   bool
        RequireOAuth         bool
        OAuthAudience        string
        ValidateToken        bool
        AuthorizationURL     string
        CertificateAuthority string
        ServerURL            string

        ConfigPath   string
        StaticConfig *config.StaticConfig

        genericiooptions.IOStreams
}

func NewMCPServerOptions(streams genericiooptions.IOStreams) *MCPServerOptions <span class="cov10" title="18">{
        return &amp;MCPServerOptions{
                IOStreams:    streams,
                Profile:      "full",
                ListOutput:   "table",
                StaticConfig: &amp;config.StaticConfig{},
        }
}</span>

func NewMCPServer(streams genericiooptions.IOStreams) *cobra.Command <span class="cov10" title="18">{
        o := NewMCPServerOptions(streams)
        cmd := &amp;cobra.Command{
                Use:     "kubernetes-mcp-server [command] [options]",
                Short:   "Kubernetes Model Context Protocol (MCP) server",
                Long:    long,
                Example: examples,
                RunE: func(c *cobra.Command, args []string) error </span><span class="cov9" title="16">{
                        if err := o.Complete(c); err != nil </span><span class="cov1" title="1">{
                                return err
                        }</span>
                        <span class="cov9" title="15">if err := o.Validate(); err != nil </span><span class="cov1" title="1">{
                                return err
                        }</span>
                        <span class="cov9" title="14">if err := o.Run(); err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>

                        <span class="cov9" title="14">return nil</span>
                },
        }

        <span class="cov10" title="18">cmd.Flags().BoolVar(&amp;o.Version, "version", o.Version, "Print version information and quit")
        cmd.Flags().IntVar(&amp;o.LogLevel, "log-level", o.LogLevel, "Set the log level (from 0 to 9)")
        cmd.Flags().StringVar(&amp;o.ConfigPath, "config", o.ConfigPath, "Path of the config file. Each profile has its set of defaults.")
        cmd.Flags().IntVar(&amp;o.SSEPort, "sse-port", o.SSEPort, "Start a SSE server on the specified port")
        cmd.Flag("sse-port").Deprecated = "Use --port instead"
        cmd.Flags().IntVar(&amp;o.HttpPort, "http-port", o.HttpPort, "Start a streamable HTTP server on the specified port")
        cmd.Flag("http-port").Deprecated = "Use --port instead"
        cmd.Flags().StringVar(&amp;o.Port, "port", o.Port, "Start a streamable HTTP and SSE HTTP server on the specified port (e.g. 8080)")
        cmd.Flags().StringVar(&amp;o.SSEBaseUrl, "sse-base-url", o.SSEBaseUrl, "SSE public base URL to use when sending the endpoint message (e.g. https://example.com)")
        cmd.Flags().StringVar(&amp;o.Kubeconfig, "kubeconfig", o.Kubeconfig, "Path to the kubeconfig file to use for authentication")
        cmd.Flags().StringVar(&amp;o.KubeconfigDir, "kubeconfig-dir", o.KubeconfigDir, "Directory containing multiple kubeconfig files for multi-cluster support")
        cmd.Flags().StringVar(&amp;o.Profile, "profile", o.Profile, "MCP profile to use (one of: "+strings.Join(mcp.ProfileNames, ", ")+")")
        cmd.Flags().StringVar(&amp;o.ListOutput, "list-output", o.ListOutput, "Output format for resource list operations (one of: "+strings.Join(output.Names, ", ")+"). Defaults to table.")
        cmd.Flags().BoolVar(&amp;o.ReadOnly, "read-only", o.ReadOnly, "If true, only tools annotated with readOnlyHint=true are exposed")
        cmd.Flags().BoolVar(&amp;o.DisableDestructive, "disable-destructive", o.DisableDestructive, "If true, tools annotated with destructiveHint=true are disabled")
        cmd.Flags().BoolVar(&amp;o.RequireOAuth, "require-oauth", o.RequireOAuth, "If true, requires OAuth authorization as defined in the Model Context Protocol (MCP) specification. This flag is ignored if transport type is stdio")
        _ = cmd.Flags().MarkHidden("require-oauth")
        cmd.Flags().StringVar(&amp;o.OAuthAudience, "oauth-audience", o.OAuthAudience, "OAuth audience for token claims validation. Optional. If not set, the audience is not validated. Only valid if require-oauth is enabled.")
        _ = cmd.Flags().MarkHidden("oauth-audience")
        cmd.Flags().BoolVar(&amp;o.ValidateToken, "validate-token", o.ValidateToken, "If true, validates the token against the Kubernetes API Server using TokenReview. Optional. If not set, the token is not validated. Only valid if require-oauth is enabled.")
        _ = cmd.Flags().MarkHidden("validate-token")
        cmd.Flags().StringVar(&amp;o.AuthorizationURL, "authorization-url", o.AuthorizationURL, "OAuth authorization server URL for protected resource endpoint. If not provided, the Kubernetes API server host will be used. Only valid if require-oauth is enabled.")
        _ = cmd.Flags().MarkHidden("authorization-url")
        cmd.Flags().StringVar(&amp;o.ServerURL, "server-url", o.ServerURL, "Server URL of this application. Optional. If set, this url will be served in protected resource metadata endpoint and tokens will be validated with this audience. If not set, expected audience is kubernetes-mcp-server. Only valid if require-oauth is enabled.")
        _ = cmd.Flags().MarkHidden("server-url")
        cmd.Flags().StringVar(&amp;o.CertificateAuthority, "certificate-authority", o.CertificateAuthority, "Certificate authority path to verify certificates. Optional. Only valid if require-oauth is enabled.")
        _ = cmd.Flags().MarkHidden("certificate-authority")

        return cmd</span>
}

func (m *MCPServerOptions) Complete(cmd *cobra.Command) error <span class="cov9" title="16">{
        if m.ConfigPath != "" </span><span class="cov5" title="4">{
                cnf, err := config.ReadConfig(m.ConfigPath)
                if err != nil </span><span class="cov1" title="1">{
                        return err
                }</span>
                <span class="cov4" title="3">m.StaticConfig = cnf</span>
        }

        <span class="cov9" title="15">m.loadFlags(cmd)

        m.initializeLogging()

        if m.StaticConfig.RequireOAuth &amp;&amp; m.StaticConfig.Port == "" </span><span class="cov0" title="0">{
                // RequireOAuth is not relevant flow for STDIO transport
                m.StaticConfig.RequireOAuth = false
        }</span>

        <span class="cov9" title="15">return nil</span>
}

func (m *MCPServerOptions) loadFlags(cmd *cobra.Command) <span class="cov9" title="15">{
        if cmd.Flag("log-level").Changed </span><span class="cov8" title="10">{
                m.StaticConfig.LogLevel = m.LogLevel
        }</span>
        <span class="cov9" title="15">if cmd.Flag("port").Changed </span><span class="cov3" title="2">{
                m.StaticConfig.Port = m.Port
        }</span> else<span class="cov8" title="13"> if cmd.Flag("sse-port").Changed </span><span class="cov0" title="0">{
                m.StaticConfig.Port = strconv.Itoa(m.SSEPort)
        }</span> else<span class="cov8" title="13"> if cmd.Flag("http-port").Changed </span><span class="cov0" title="0">{
                m.StaticConfig.Port = strconv.Itoa(m.HttpPort)
        }</span>
        <span class="cov9" title="15">if cmd.Flag("sse-base-url").Changed </span><span class="cov0" title="0">{
                m.StaticConfig.SSEBaseURL = m.SSEBaseUrl
        }</span>
        <span class="cov9" title="15">if cmd.Flag("kubeconfig").Changed </span><span class="cov0" title="0">{
                m.StaticConfig.KubeConfig = m.Kubeconfig
        }</span>
        <span class="cov9" title="15">if cmd.Flag("kubeconfig-dir").Changed </span><span class="cov0" title="0">{
                m.StaticConfig.KubeConfigDir = m.KubeconfigDir
        }</span>
        <span class="cov9" title="15">if cmd.Flag("list-output").Changed || m.StaticConfig.ListOutput == "" </span><span class="cov9" title="14">{
                m.StaticConfig.ListOutput = m.ListOutput
        }</span>
        <span class="cov9" title="15">if cmd.Flag("read-only").Changed </span><span class="cov3" title="2">{
                m.StaticConfig.ReadOnly = m.ReadOnly
        }</span>
        <span class="cov9" title="15">if cmd.Flag("disable-destructive").Changed </span><span class="cov3" title="2">{
                m.StaticConfig.DisableDestructive = m.DisableDestructive
        }</span>
        <span class="cov9" title="15">if cmd.Flag("require-oauth").Changed </span><span class="cov3" title="2">{
                m.StaticConfig.RequireOAuth = m.RequireOAuth
        }</span>
        <span class="cov9" title="15">if cmd.Flag("oauth-audience").Changed </span><span class="cov0" title="0">{
                m.StaticConfig.OAuthAudience = m.OAuthAudience
        }</span>
        <span class="cov9" title="15">if cmd.Flag("validate-token").Changed </span><span class="cov0" title="0">{
                m.StaticConfig.ValidateToken = m.ValidateToken
        }</span>
        <span class="cov9" title="15">if cmd.Flag("authorization-url").Changed </span><span class="cov3" title="2">{
                m.StaticConfig.AuthorizationURL = m.AuthorizationURL
        }</span>
        <span class="cov9" title="15">if cmd.Flag("server-url").Changed </span><span class="cov3" title="2">{
                m.StaticConfig.ServerURL = m.ServerURL
        }</span>
        <span class="cov9" title="15">if cmd.Flag("certificate-authority").Changed </span><span class="cov0" title="0">{
                m.StaticConfig.CertificateAuthority = m.CertificateAuthority
        }</span>
}

func (m *MCPServerOptions) initializeLogging() <span class="cov9" title="15">{
        flagSet := flag.NewFlagSet("klog", flag.ContinueOnError)
        klog.InitFlags(flagSet)

        // Use stderr for logging in STDIO mode to avoid interfering with JSON-RPC on stdout
        logOutput := m.ErrOut
        if logOutput == nil </span><span class="cov0" title="0">{
                logOutput = m.Out // Fallback to Out if ErrOut is not available
        }</span>
        // In STDIO mode (no port specified), always use stderr for logs
        <span class="cov9" title="15">if m.StaticConfig.Port == "" </span><span class="cov8" title="11">{
                logOutput = m.ErrOut
        }</span>

        <span class="cov9" title="15">loggerOptions := []textlogger.ConfigOption{textlogger.Output(logOutput)}
        if m.StaticConfig.LogLevel &gt;= 0 </span><span class="cov9" title="15">{
                loggerOptions = append(loggerOptions, textlogger.Verbosity(m.StaticConfig.LogLevel))
                _ = flagSet.Parse([]string{"--v", strconv.Itoa(m.StaticConfig.LogLevel)})
        }</span>
        <span class="cov9" title="15">logger := textlogger.NewLogger(textlogger.NewConfig(loggerOptions...))
        klog.SetLoggerWithOptions(logger)</span>
}

func (m *MCPServerOptions) Validate() error <span class="cov9" title="15">{
        if m.Port != "" &amp;&amp; (m.SSEPort &gt; 0 || m.HttpPort &gt; 0) </span><span class="cov0" title="0">{
                return fmt.Errorf("--port is mutually exclusive with deprecated --http-port and --sse-port flags")
        }</span>
        <span class="cov9" title="15">if !m.StaticConfig.RequireOAuth &amp;&amp; (m.StaticConfig.ValidateToken || m.StaticConfig.OAuthAudience != "" || m.StaticConfig.AuthorizationURL != "" || m.StaticConfig.ServerURL != "" || m.StaticConfig.CertificateAuthority != "") </span><span class="cov0" title="0">{
                return fmt.Errorf("validate-token, oauth-audience, authorization-url, server-url and certificate-authority are only valid if require-oauth is enabled. Missing --port may implicitly set require-oauth to false")
        }</span>
        <span class="cov9" title="15">if m.StaticConfig.AuthorizationURL != "" </span><span class="cov3" title="2">{
                u, err := url.Parse(m.StaticConfig.AuthorizationURL)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov3" title="2">if u.Scheme != "https" &amp;&amp; u.Scheme != "http" </span><span class="cov1" title="1">{
                        return fmt.Errorf("--authorization-url must be a valid URL")
                }</span>
                <span class="cov1" title="1">if u.Scheme == "http" </span><span class="cov0" title="0">{
                        klog.Warningf("authorization-url is using http://, this is not recommended production use")
                }</span>
        }
        <span class="cov9" title="14">return nil</span>
}

func (m *MCPServerOptions) Run() error <span class="cov9" title="14">{
        profile := mcp.ProfileFromString(m.Profile)
        if profile == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("invalid profile name: %s, valid names are: %s", m.Profile, strings.Join(mcp.ProfileNames, ", "))
        }</span>
        <span class="cov9" title="14">listOutput := output.FromString(m.StaticConfig.ListOutput)
        if listOutput == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("invalid output name: %s, valid names are: %s", m.StaticConfig.ListOutput, strings.Join(output.Names, ", "))
        }</span>
        <span class="cov9" title="14">klog.V(1).Info("Starting kubernetes-mcp-server")
        klog.V(1).Infof(" - Config: %s", m.ConfigPath)
        klog.V(1).Infof(" - Profile: %s", profile.GetName())
        klog.V(1).Infof(" - ListOutput: %s", listOutput.GetName())
        klog.V(1).Infof(" - Read-only mode: %t", m.StaticConfig.ReadOnly)
        klog.V(1).Infof(" - Disable destructive tools: %t", m.StaticConfig.DisableDestructive)

        if m.Version </span><span class="cov9" title="14">{
                _, _ = fmt.Fprintf(m.Out, "%s\n", version.Version)
                return nil
        }</span>

        <span class="cov0" title="0">var oidcProvider *oidc.Provider
        if m.StaticConfig.AuthorizationURL != "" </span><span class="cov0" title="0">{
                ctx := context.Background()
                if m.StaticConfig.CertificateAuthority != "" </span><span class="cov0" title="0">{
                        httpClient := &amp;http.Client{}
                        caCert, err := os.ReadFile(m.StaticConfig.CertificateAuthority)
                        if err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed to read CA certificate from %s: %w", m.StaticConfig.CertificateAuthority, err)
                        }</span>
                        <span class="cov0" title="0">caCertPool := x509.NewCertPool()
                        if !caCertPool.AppendCertsFromPEM(caCert) </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed to append CA certificate from %s to pool", m.StaticConfig.CertificateAuthority)
                        }</span>

                        <span class="cov0" title="0">if caCertPool.Equal(x509.NewCertPool()) </span><span class="cov0" title="0">{
                                caCertPool = nil
                        }</span>

                        <span class="cov0" title="0">transport := &amp;http.Transport{
                                TLSClientConfig: &amp;tls.Config{
                                        RootCAs: caCertPool,
                                },
                        }
                        httpClient.Transport = transport
                        ctx = oidc.ClientContext(ctx, httpClient)</span>
                }
                <span class="cov0" title="0">provider, err := oidc.NewProvider(ctx, m.StaticConfig.AuthorizationURL)
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("unable to setup OIDC provider: %w", err)
                }</span>
                <span class="cov0" title="0">oidcProvider = provider</span>
        }

        <span class="cov0" title="0">mcpServer, err := mcp.NewServer(mcp.Configuration{
                Profile:      profile,
                ListOutput:   listOutput,
                StaticConfig: m.StaticConfig,
        })
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to initialize MCP server: %w", err)
        }</span>
        <span class="cov0" title="0">defer mcpServer.Close()

        if m.StaticConfig.Port != "" </span><span class="cov0" title="0">{
                ctx := context.Background()
                return internalhttp.Serve(ctx, mcpServer, m.StaticConfig, oidcProvider)
        }</span>

        <span class="cov0" title="0">if err := mcpServer.ServeStdio(); err != nil &amp;&amp; !errors.Is(err, context.Canceled) </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov0" title="0">return nil</span>
}
</pre>
		
		<pre class="file" id="file8" style="display: none">package kubernetes

import (
        "fmt"

        "k8s.io/apimachinery/pkg/runtime/schema"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config"
)

// isAllowed checks the resource is in denied list or not.
// If it is in denied list, this function returns false.
func isAllowed(
        staticConfig *config.StaticConfig, // TODO: maybe just use the denied resource slice
        gvk *schema.GroupVersionKind,
) bool <span class="cov0" title="0">{
        if staticConfig == nil </span><span class="cov0" title="0">{
                return true
        }</span>

        <span class="cov0" title="0">for _, val := range staticConfig.DeniedResources </span><span class="cov0" title="0">{
                // If kind is empty, that means Group/Version pair is denied entirely
                if val.Kind == "" </span><span class="cov0" title="0">{
                        if gvk.Group == val.Group &amp;&amp; gvk.Version == val.Version </span><span class="cov0" title="0">{
                                return false
                        }</span>
                }
                <span class="cov0" title="0">if gvk.Group == val.Group &amp;&amp;
                        gvk.Version == val.Version &amp;&amp;
                        gvk.Kind == val.Kind </span><span class="cov0" title="0">{
                        return false
                }</span>
        }

        <span class="cov0" title="0">return true</span>
}

func isNotAllowedError(gvk *schema.GroupVersionKind) error <span class="cov0" title="0">{
        return fmt.Errorf("resource not allowed: %s", gvk.String())
}</span>
</pre>
		
		<pre class="file" id="file9" style="display: none">package kubernetes

import (
        "context"
        "fmt"

        authenticationv1api "k8s.io/api/authentication/v1"
        authorizationv1api "k8s.io/api/authorization/v1"
        v1 "k8s.io/api/core/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/runtime/schema"
        "k8s.io/apimachinery/pkg/util/httpstream"
        "k8s.io/client-go/discovery"
        "k8s.io/client-go/kubernetes"
        authenticationv1 "k8s.io/client-go/kubernetes/typed/authentication/v1"
        authorizationv1 "k8s.io/client-go/kubernetes/typed/authorization/v1"
        corev1 "k8s.io/client-go/kubernetes/typed/core/v1"
        "k8s.io/client-go/rest"
        "k8s.io/client-go/tools/remotecommand"
        "k8s.io/metrics/pkg/apis/metrics"
        metricsv1beta1api "k8s.io/metrics/pkg/apis/metrics/v1beta1"
        metricsv1beta1 "k8s.io/metrics/pkg/client/clientset/versioned/typed/metrics/v1beta1"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config"
)

// AccessControlClientset is a limited clientset delegating interface to the standard kubernetes.Clientset
// Only a limited set of functions are implemented with a single point of access to the kubernetes API where
// apiVersion and kinds are checked for allowed access
type AccessControlClientset struct {
        cfg             *rest.Config
        delegate        kubernetes.Interface
        discoveryClient discovery.DiscoveryInterface
        metricsV1beta1  *metricsv1beta1.MetricsV1beta1Client
        staticConfig    *config.StaticConfig // TODO: maybe just store the denied resource slice
}

func (a *AccessControlClientset) DiscoveryClient() discovery.DiscoveryInterface <span class="cov10" title="25">{
        return a.discoveryClient
}</span>

func (a *AccessControlClientset) Pods(namespace string) (corev1.PodInterface, error) <span class="cov0" title="0">{
        gvk := &amp;schema.GroupVersionKind{Group: "", Version: "v1", Kind: "Pod"}
        if !isAllowed(a.staticConfig, gvk) </span><span class="cov0" title="0">{
                return nil, isNotAllowedError(gvk)
        }</span>
        <span class="cov0" title="0">return a.delegate.CoreV1().Pods(namespace), nil</span>
}

func (a *AccessControlClientset) PodsExec(namespace, name string, podExecOptions *v1.PodExecOptions) (remotecommand.Executor, error) <span class="cov0" title="0">{
        gvk := &amp;schema.GroupVersionKind{Group: "", Version: "v1", Kind: "Pod"}
        if !isAllowed(a.staticConfig, gvk) </span><span class="cov0" title="0">{
                return nil, isNotAllowedError(gvk)
        }</span>
        // Compute URL
        // https://github.com/kubernetes/kubectl/blob/5366de04e168bcbc11f5e340d131a9ca8b7d0df4/pkg/cmd/exec/exec.go#L382-L397
        <span class="cov0" title="0">execRequest := a.delegate.CoreV1().RESTClient().
                Post().
                Resource("pods").
                Namespace(namespace).
                Name(name).
                SubResource("exec")
        execRequest.VersionedParams(podExecOptions, ParameterCodec)
        spdyExec, err := remotecommand.NewSPDYExecutor(a.cfg, "POST", execRequest.URL())
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov0" title="0">webSocketExec, err := remotecommand.NewWebSocketExecutor(a.cfg, "GET", execRequest.URL().String())
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov0" title="0">return remotecommand.NewFallbackExecutor(webSocketExec, spdyExec, func(err error) bool </span><span class="cov0" title="0">{
                return httpstream.IsUpgradeFailure(err) || httpstream.IsHTTPSProxyError(err)
        }</span>)
}

func (a *AccessControlClientset) PodsMetricses(ctx context.Context, namespace, name string, listOptions metav1.ListOptions) (*metrics.PodMetricsList, error) <span class="cov0" title="0">{
        gvk := &amp;schema.GroupVersionKind{Group: metrics.GroupName, Version: metricsv1beta1api.SchemeGroupVersion.Version, Kind: "PodMetrics"}
        if !isAllowed(a.staticConfig, gvk) </span><span class="cov0" title="0">{
                return nil, isNotAllowedError(gvk)
        }</span>
        <span class="cov0" title="0">versionedMetrics := &amp;metricsv1beta1api.PodMetricsList{}
        var err error
        if name != "" </span><span class="cov0" title="0">{
                m, err := a.metricsV1beta1.PodMetricses(namespace).Get(ctx, name, metav1.GetOptions{})
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to get metrics for pod %s/%s: %w", namespace, name, err)
                }</span>
                <span class="cov0" title="0">versionedMetrics.Items = []metricsv1beta1api.PodMetrics{*m}</span>
        } else<span class="cov0" title="0"> {
                versionedMetrics, err = a.metricsV1beta1.PodMetricses(namespace).List(ctx, listOptions)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to list pod metrics in namespace %s: %w", namespace, err)
                }</span>
        }
        <span class="cov0" title="0">convertedMetrics := &amp;metrics.PodMetricsList{}
        return convertedMetrics, metricsv1beta1api.Convert_v1beta1_PodMetricsList_To_metrics_PodMetricsList(versionedMetrics, convertedMetrics, nil)</span>
}

func (a *AccessControlClientset) Services(namespace string) (corev1.ServiceInterface, error) <span class="cov0" title="0">{
        gvk := &amp;schema.GroupVersionKind{Group: "", Version: "v1", Kind: "Service"}
        if !isAllowed(a.staticConfig, gvk) </span><span class="cov0" title="0">{
                return nil, isNotAllowedError(gvk)
        }</span>
        <span class="cov0" title="0">return a.delegate.CoreV1().Services(namespace), nil</span>
}

func (a *AccessControlClientset) SelfSubjectAccessReviews() (authorizationv1.SelfSubjectAccessReviewInterface, error) <span class="cov0" title="0">{
        gvk := &amp;schema.GroupVersionKind{Group: authorizationv1api.GroupName, Version: authorizationv1api.SchemeGroupVersion.Version, Kind: "SelfSubjectAccessReview"}
        if !isAllowed(a.staticConfig, gvk) </span><span class="cov0" title="0">{
                return nil, isNotAllowedError(gvk)
        }</span>
        <span class="cov0" title="0">return a.delegate.AuthorizationV1().SelfSubjectAccessReviews(), nil</span>
}

// TokenReview returns TokenReviewInterface
func (a *AccessControlClientset) TokenReview() (authenticationv1.TokenReviewInterface, error) <span class="cov0" title="0">{
        gvk := &amp;schema.GroupVersionKind{Group: authenticationv1api.GroupName, Version: authorizationv1api.SchemeGroupVersion.Version, Kind: "TokenReview"}
        if !isAllowed(a.staticConfig, gvk) </span><span class="cov0" title="0">{
                return nil, isNotAllowedError(gvk)
        }</span>
        <span class="cov0" title="0">return a.delegate.AuthenticationV1().TokenReviews(), nil</span>
}

func NewAccessControlClientset(cfg *rest.Config, staticConfig *config.StaticConfig) (*AccessControlClientset, error) <span class="cov10" title="25">{
        clientSet, err := kubernetes.NewForConfig(cfg)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov10" title="25">metricsClient, err := metricsv1beta1.NewForConfig(cfg)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov10" title="25">return &amp;AccessControlClientset{
                cfg:             cfg,
                delegate:        clientSet,
                discoveryClient: clientSet.DiscoveryClient,
                metricsV1beta1:  metricsClient,
                staticConfig:    staticConfig,
        }, nil</span>
}
</pre>
		
		<pre class="file" id="file10" style="display: none">package kubernetes

import (
        "k8s.io/apimachinery/pkg/api/meta"
        "k8s.io/apimachinery/pkg/runtime/schema"
        "k8s.io/client-go/restmapper"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config"
)

type AccessControlRESTMapper struct {
        delegate     *restmapper.DeferredDiscoveryRESTMapper
        staticConfig *config.StaticConfig // TODO: maybe just store the denied resource slice
}

var _ meta.RESTMapper = &amp;AccessControlRESTMapper{}

func (a AccessControlRESTMapper) KindFor(resource schema.GroupVersionResource) (schema.GroupVersionKind, error) <span class="cov0" title="0">{
        gvk, err := a.delegate.KindFor(resource)
        if err != nil </span><span class="cov0" title="0">{
                return schema.GroupVersionKind{}, err
        }</span>
        <span class="cov0" title="0">if !isAllowed(a.staticConfig, &amp;gvk) </span><span class="cov0" title="0">{
                return schema.GroupVersionKind{}, isNotAllowedError(&amp;gvk)
        }</span>
        <span class="cov0" title="0">return gvk, nil</span>
}

func (a AccessControlRESTMapper) KindsFor(resource schema.GroupVersionResource) ([]schema.GroupVersionKind, error) <span class="cov0" title="0">{
        gvks, err := a.delegate.KindsFor(resource)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov0" title="0">for i := range gvks </span><span class="cov0" title="0">{
                if !isAllowed(a.staticConfig, &amp;gvks[i]) </span><span class="cov0" title="0">{
                        return nil, isNotAllowedError(&amp;gvks[i])
                }</span>
        }
        <span class="cov0" title="0">return gvks, nil</span>
}

func (a AccessControlRESTMapper) ResourceFor(input schema.GroupVersionResource) (schema.GroupVersionResource, error) <span class="cov0" title="0">{
        return a.delegate.ResourceFor(input)
}</span>

func (a AccessControlRESTMapper) ResourcesFor(input schema.GroupVersionResource) ([]schema.GroupVersionResource, error) <span class="cov0" title="0">{
        return a.delegate.ResourcesFor(input)
}</span>

func (a AccessControlRESTMapper) RESTMapping(gk schema.GroupKind, versions ...string) (*meta.RESTMapping, error) <span class="cov0" title="0">{
        for _, version := range versions </span><span class="cov0" title="0">{
                gvk := &amp;schema.GroupVersionKind{Group: gk.Group, Version: version, Kind: gk.Kind}
                if !isAllowed(a.staticConfig, gvk) </span><span class="cov0" title="0">{
                        return nil, isNotAllowedError(gvk)
                }</span>
        }
        <span class="cov0" title="0">return a.delegate.RESTMapping(gk, versions...)</span>
}

func (a AccessControlRESTMapper) RESTMappings(gk schema.GroupKind, versions ...string) ([]*meta.RESTMapping, error) <span class="cov0" title="0">{
        for _, version := range versions </span><span class="cov0" title="0">{
                gvk := &amp;schema.GroupVersionKind{Group: gk.Group, Version: version, Kind: gk.Kind}
                if !isAllowed(a.staticConfig, gvk) </span><span class="cov0" title="0">{
                        return nil, isNotAllowedError(gvk)
                }</span>
        }
        <span class="cov0" title="0">return a.delegate.RESTMappings(gk, versions...)</span>
}

func (a AccessControlRESTMapper) ResourceSingularizer(resource string) (singular string, err error) <span class="cov0" title="0">{
        return a.delegate.ResourceSingularizer(resource)
}</span>

func (a AccessControlRESTMapper) Reset() <span class="cov0" title="0">{
        a.delegate.Reset()
}</span>

func NewAccessControlRESTMapper(delegate *restmapper.DeferredDiscoveryRESTMapper, staticConfig *config.StaticConfig) *AccessControlRESTMapper <span class="cov10" title="25">{
        return &amp;AccessControlRESTMapper{delegate: delegate, staticConfig: staticConfig}
}</span>
</pre>
		
		<pre class="file" id="file11" style="display: none">package kubernetes

import (
        "k8s.io/apimachinery/pkg/runtime"
        "k8s.io/client-go/rest"
        "k8s.io/client-go/tools/clientcmd"
        clientcmdapi "k8s.io/client-go/tools/clientcmd/api"
        "k8s.io/client-go/tools/clientcmd/api/latest"
)

// InClusterConfig is a variable that holds the function to get the in-cluster config
// Exposed for testing
var InClusterConfig = func() (*rest.Config, error) <span class="cov0" title="0">{
        // TODO use kubernetes.default.svc instead of resolved server
        // Currently running into: `http: server gave HTTP response to HTTPS client`
        inClusterConfig, err := rest.InClusterConfig()
        if inClusterConfig != nil </span><span class="cov0" title="0">{
                inClusterConfig.Host = "https://kubernetes.default.svc"
        }</span>
        <span class="cov0" title="0">return inClusterConfig, err</span>
}

// resolveKubernetesConfigurations resolves the required kubernetes configurations and sets them in the Kubernetes struct
func resolveKubernetesConfigurations(kubernetes *Manager) error <span class="cov5" title="26">{
        // Always set clientCmdConfig
        pathOptions := clientcmd.NewDefaultPathOptions()
        if kubernetes.staticConfig.KubeConfig != "" </span><span class="cov5" title="26">{
                pathOptions.LoadingRules.ExplicitPath = kubernetes.staticConfig.KubeConfig
        }</span>
        <span class="cov5" title="26">kubernetes.clientCmdConfig = clientcmd.NewNonInteractiveDeferredLoadingClientConfig(
                pathOptions.LoadingRules,
                &amp;clientcmd.ConfigOverrides{ClusterInfo: clientcmdapi.Cluster{Server: ""}})
        var err error
        if kubernetes.IsInCluster() </span><span class="cov0" title="0">{
                kubernetes.cfg, err = InClusterConfig()
                if err == nil &amp;&amp; kubernetes.cfg != nil </span><span class="cov0" title="0">{
                        return nil
                }</span>
        }
        // Out of cluster
        <span class="cov5" title="26">kubernetes.cfg, err = kubernetes.clientCmdConfig.ClientConfig()
        if kubernetes.cfg != nil &amp;&amp; kubernetes.cfg.UserAgent == "" </span><span class="cov5" title="24">{
                kubernetes.cfg.UserAgent = rest.DefaultKubernetesUserAgent()
        }</span>
        <span class="cov5" title="26">return err</span>
}

func (m *Manager) IsInCluster() bool <span class="cov5" title="30">{
        if m.staticConfig.KubeConfig != "" </span><span class="cov5" title="27">{
                return false
        }</span>
        <span class="cov2" title="3">cfg, err := InClusterConfig()
        return err == nil &amp;&amp; cfg != nil</span>
}

func (m *Manager) configuredNamespace() string <span class="cov0" title="0">{
        if ns, _, nsErr := m.clientCmdConfig.Namespace(); nsErr == nil </span><span class="cov0" title="0">{
                return ns
        }</span>
        <span class="cov0" title="0">return ""</span>
}

func (m *Manager) NamespaceOrDefault(namespace string) string <span class="cov0" title="0">{
        if namespace == "" </span><span class="cov0" title="0">{
                return m.configuredNamespace()
        }</span>
        <span class="cov0" title="0">return namespace</span>
}

func (k *Kubernetes) NamespaceOrDefault(namespace string) string <span class="cov0" title="0">{
        return k.manager.NamespaceOrDefault(namespace)
}</span>

// ToRESTConfig returns the rest.Config object (genericclioptions.RESTClientGetter)
func (m *Manager) ToRESTConfig() (*rest.Config, error) <span class="cov10" title="1011">{
        return m.cfg, nil
}</span>

// ToRawKubeConfigLoader returns the clientcmd.ClientConfig object (genericclioptions.RESTClientGetter)
func (m *Manager) ToRawKubeConfigLoader() clientcmd.ClientConfig <span class="cov0" title="0">{
        return m.clientCmdConfig
}</span>

func (m *Manager) ConfigurationView(minify bool) (runtime.Object, error) <span class="cov0" title="0">{
        var cfg clientcmdapi.Config
        var err error
        if m.IsInCluster() </span><span class="cov0" title="0">{
                cfg = *clientcmdapi.NewConfig()
                cfg.Clusters["cluster"] = &amp;clientcmdapi.Cluster{
                        Server:                m.cfg.Host,
                        InsecureSkipTLSVerify: m.cfg.Insecure,
                }
                cfg.AuthInfos["user"] = &amp;clientcmdapi.AuthInfo{
                        Token: m.cfg.BearerToken,
                }
                cfg.Contexts["context"] = &amp;clientcmdapi.Context{
                        Cluster:  "cluster",
                        AuthInfo: "user",
                }
                cfg.CurrentContext = "context"
        }</span> else<span class="cov0" title="0"> if cfg, err = m.clientCmdConfig.RawConfig(); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov0" title="0">if minify </span><span class="cov0" title="0">{
                if err = clientcmdapi.MinifyConfig(&amp;cfg); err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>
        }
        //nolint:staticcheck
        <span class="cov0" title="0">if err = clientcmdapi.FlattenConfig(&amp;cfg); err != nil </span>{<span class="cov0" title="0">
                // ignore error
                //return "", err
        }</span>
        <span class="cov0" title="0">return latest.Scheme.ConvertToVersion(&amp;cfg, latest.ExternalVersion)</span>
}
</pre>
		
		<pre class="file" id="file12" style="display: none">package kubernetes

import (
        "context"
        v1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
        "k8s.io/apimachinery/pkg/runtime"
        "k8s.io/apimachinery/pkg/runtime/schema"
        "strings"
)

func (k *Kubernetes) EventsList(ctx context.Context, namespace string) ([]map[string]any, error) <span class="cov0" title="0">{
        var eventMap []map[string]any
        raw, err := k.ResourcesList(ctx, &amp;schema.GroupVersionKind{
                Group: "", Version: "v1", Kind: "Event",
        }, namespace, ResourceListOptions{})
        if err != nil </span><span class="cov0" title="0">{
                return eventMap, err
        }</span>
        <span class="cov0" title="0">unstructuredList := raw.(*unstructured.UnstructuredList)
        if len(unstructuredList.Items) == 0 </span><span class="cov0" title="0">{
                return eventMap, nil
        }</span>
        <span class="cov0" title="0">for _, item := range unstructuredList.Items </span><span class="cov0" title="0">{
                event := &amp;v1.Event{}
                if err = runtime.DefaultUnstructuredConverter.FromUnstructured(item.Object, event); err != nil </span><span class="cov0" title="0">{
                        return eventMap, err
                }</span>
                <span class="cov0" title="0">timestamp := event.EventTime.Time
                if timestamp.IsZero() &amp;&amp; event.Series != nil </span><span class="cov0" title="0">{
                        timestamp = event.Series.LastObservedTime.Time
                }</span> else<span class="cov0" title="0"> if timestamp.IsZero() &amp;&amp; event.Count &gt; 1 </span><span class="cov0" title="0">{
                        timestamp = event.LastTimestamp.Time
                }</span> else<span class="cov0" title="0"> if timestamp.IsZero() </span><span class="cov0" title="0">{
                        timestamp = event.FirstTimestamp.Time
                }</span>
                <span class="cov0" title="0">eventMap = append(eventMap, map[string]any{
                        "Namespace": event.Namespace,
                        "Timestamp": timestamp.String(),
                        "Type":      event.Type,
                        "Reason":    event.Reason,
                        "InvolvedObject": map[string]string{
                                "apiVersion": event.InvolvedObject.APIVersion,
                                "Kind":       event.InvolvedObject.Kind,
                                "Name":       event.InvolvedObject.Name,
                        },
                        "Message": strings.TrimSpace(event.Message),
                })</span>
        }
        <span class="cov0" title="0">return eventMap, nil</span>
}
</pre>
		
		<pre class="file" id="file13" style="display: none">package kubernetes

import (
        "context"
        "sync"
        "time"

        "k8s.io/klog/v2"
)

// ClusterHealthState represents the health state of a cluster
type ClusterHealthState int

const (
        ClusterHealthUnknown ClusterHealthState = iota
        ClusterHealthHealthy
        ClusterHealthDegraded
        ClusterHealthUnhealthy
)

func (s ClusterHealthState) String() string <span class="cov2" title="5">{
        switch s </span>{
        case ClusterHealthHealthy:<span class="cov1" title="1">
                return "healthy"</span>
        case ClusterHealthDegraded:<span class="cov1" title="1">
                return "degraded"</span>
        case ClusterHealthUnhealthy:<span class="cov1" title="1">
                return "unhealthy"</span>
        default:<span class="cov1" title="2">
                return "unknown"</span>
        }
}

// CircuitBreakerState represents the state of the circuit breaker
type CircuitBreakerState int

const (
        CircuitBreakerClosed CircuitBreakerState = iota
        CircuitBreakerOpen
        CircuitBreakerHalfOpen
)

func (s CircuitBreakerState) String() string <span class="cov2" title="4">{
        switch s </span>{
        case CircuitBreakerClosed:<span class="cov1" title="1">
                return "closed"</span>
        case CircuitBreakerOpen:<span class="cov1" title="1">
                return "open"</span>
        case CircuitBreakerHalfOpen:<span class="cov1" title="1">
                return "half-open"</span>
        default:<span class="cov1" title="1">
                return "unknown"</span>
        }
}

// ClusterHealthConfig configuration for cluster health monitoring
type ClusterHealthConfig struct {
        MaxFailures     int           // Maximum failures before opening circuit
        CooldownTime    time.Duration // Time to wait before attempting half-open
        HealthCheckTime time.Duration // Interval between health checks
        RequestTimeout  time.Duration // Timeout for health check requests
}

// DefaultClusterHealthConfig returns default health configuration
func DefaultClusterHealthConfig() ClusterHealthConfig <span class="cov3" title="9">{
        return ClusterHealthConfig{
                MaxFailures:     3,
                CooldownTime:    30 * time.Second,
                HealthCheckTime: 10 * time.Second,
                RequestTimeout:  5 * time.Second,
        }
}</span>

// ClusterHealthStatus contains the health status of a cluster
type ClusterHealthStatus struct {
        Cluster      string              `json:"cluster"`
        State        ClusterHealthState  `json:"state"`
        CircuitState CircuitBreakerState `json:"circuit_state"`
        FailureCount int                 `json:"failure_count"`
        LastCheck    time.Time           `json:"last_check"`
        LastFailure  time.Time           `json:"last_failure,omitempty"`
        LastSuccess  time.Time           `json:"last_success,omitempty"`
        ErrorMessage string              `json:"error_message,omitempty"`
}

// ClusterHealthMonitor monitors cluster health with circuit breaker pattern
type ClusterHealthMonitor struct {
        config      ClusterHealthConfig
        statuses    map[string]*ClusterHealthStatus
        mutex       sync.RWMutex
        logger      klog.Logger
        stopCh      chan struct{}
        healthCheck func(ctx context.Context, cluster string) error
}

// NewClusterHealthMonitor creates a new cluster health monitor
func NewClusterHealthMonitor(config ClusterHealthConfig, logger klog.Logger, healthCheck func(ctx context.Context, cluster string) error) *ClusterHealthMonitor <span class="cov4" title="15">{
        return &amp;ClusterHealthMonitor{
                config:      config,
                statuses:    make(map[string]*ClusterHealthStatus),
                logger:      logger,
                stopCh:      make(chan struct{}),
                healthCheck: healthCheck,
        }
}</span>

// Start starts the health monitoring
func (chm *ClusterHealthMonitor) Start(ctx context.Context, clusters []string) <span class="cov3" title="11">{
        chm.logger.Info("Starting cluster health monitor", "clusters", len(clusters))

        // Initialize status for each cluster
        chm.mutex.Lock()
        for _, cluster := range clusters </span><span class="cov4" title="18">{
                if _, exists := chm.statuses[cluster]; !exists </span><span class="cov3" title="10">{
                        chm.statuses[cluster] = &amp;ClusterHealthStatus{
                                Cluster:      cluster,
                                State:        ClusterHealthUnknown,
                                CircuitState: CircuitBreakerClosed,
                        }
                }</span>
        }
        <span class="cov3" title="11">chm.mutex.Unlock()

        // Start monitoring goroutine
        go chm.monitorLoop(ctx)</span>
}

// Stop stops the health monitoring
func (chm *ClusterHealthMonitor) Stop() <span class="cov4" title="20">{
        chm.logger.Info("Stopping cluster health monitor")
        select </span>{
        case &lt;-chm.stopCh:<span class="cov3" title="9">
                // Already stopped
                return</span>
        default:<span class="cov3" title="11">
                close(chm.stopCh)</span>
        }
}

// monitorLoop runs the health monitoring loop
func (chm *ClusterHealthMonitor) monitorLoop(ctx context.Context) <span class="cov3" title="11">{
        ticker := time.NewTicker(chm.config.HealthCheckTime)
        defer ticker.Stop()

        for </span><span class="cov5" title="61">{
                select </span>{
                case &lt;-ctx.Done():<span class="cov1" title="1">
                        return</span>
                case &lt;-chm.stopCh:<span class="cov3" title="10">
                        return</span>
                case &lt;-ticker.C:<span class="cov5" title="50">
                        chm.checkAllClusters(ctx)</span>
                }
        }
}

// checkAllClusters performs health checks on all monitored clusters
func (chm *ClusterHealthMonitor) checkAllClusters(ctx context.Context) <span class="cov5" title="50">{
        chm.mutex.RLock()
        clusters := make([]string, 0, len(chm.statuses))
        for cluster := range chm.statuses </span><span class="cov5" title="50">{
                clusters = append(clusters, cluster)
        }</span>
        <span class="cov5" title="50">chm.mutex.RUnlock()

        for _, cluster := range clusters </span><span class="cov5" title="50">{
                chm.checkClusterHealth(ctx, cluster)
        }</span>
}

// checkClusterHealth performs a health check on a specific cluster
func (chm *ClusterHealthMonitor) checkClusterHealth(ctx context.Context, cluster string) <span class="cov5" title="50">{
        chm.mutex.Lock()
        status, exists := chm.statuses[cluster]
        if !exists </span><span class="cov0" title="0">{
                chm.mutex.Unlock()
                return
        }</span>

        // Skip if circuit is open and cooldown hasn't expired
        <span class="cov5" title="50">if status.CircuitState == CircuitBreakerOpen </span><span class="cov4" title="23">{
                if time.Since(status.LastFailure) &lt; chm.config.CooldownTime </span><span class="cov4" title="17">{
                        chm.mutex.Unlock()
                        return
                }</span>
                // Move to half-open state
                <span class="cov3" title="6">status.CircuitState = CircuitBreakerHalfOpen
                chm.logger.V(2).Info("Circuit breaker moving to half-open", "cluster", cluster)</span>
        }
        <span class="cov4" title="33">chm.mutex.Unlock()

        // Perform health check with timeout
        checkCtx, cancel := context.WithTimeout(ctx, chm.config.RequestTimeout)
        defer cancel()

        err := chm.healthCheck(checkCtx, cluster)

        chm.mutex.Lock()
        defer chm.mutex.Unlock()

        status.LastCheck = time.Now()

        if err != nil </span><span class="cov3" title="10">{
                // Health check failed
                status.FailureCount++
                status.LastFailure = time.Now()
                status.ErrorMessage = err.Error()

                chm.logger.V(3).Info("Cluster health check failed",
                        "cluster", cluster,
                        "failures", status.FailureCount,
                        "error", err.Error())

                // Update state based on failure count
                if status.FailureCount &gt;= chm.config.MaxFailures </span><span class="cov3" title="7">{
                        status.State = ClusterHealthUnhealthy
                        status.CircuitState = CircuitBreakerOpen
                        chm.logger.Info("Circuit breaker opened for cluster",
                                "cluster", cluster,
                                "failures", status.FailureCount)
                }</span> else<span class="cov2" title="3"> {
                        status.State = ClusterHealthDegraded
                }</span>

                // If in half-open state and failed, go back to open
                <span class="cov3" title="10">if status.CircuitState == CircuitBreakerHalfOpen </span><span class="cov0" title="0">{
                        status.CircuitState = CircuitBreakerOpen
                        chm.logger.V(2).Info("Circuit breaker reopened from half-open", "cluster", cluster)
                }</span>
        } else<span class="cov4" title="23"> {
                // Health check succeeded
                status.LastSuccess = time.Now()
                status.ErrorMessage = ""

                if status.CircuitState == CircuitBreakerHalfOpen || status.FailureCount &gt; 0 </span><span class="cov1" title="2">{
                        chm.logger.Info("Cluster health restored", "cluster", cluster)
                }</span>

                // Reset failure count and update state
                <span class="cov4" title="23">status.FailureCount = 0
                status.State = ClusterHealthHealthy
                status.CircuitState = CircuitBreakerClosed</span>
        }
}

// IsHealthy returns true if the cluster is healthy and circuit is closed
func (chm *ClusterHealthMonitor) IsHealthy(cluster string) bool <span class="cov9" title="2895">{
        chm.mutex.RLock()
        defer chm.mutex.RUnlock()

        status, exists := chm.statuses[cluster]
        if !exists </span><span class="cov0" title="0">{
                return false
        }</span>

        <span class="cov9" title="2807">return status.State == ClusterHealthHealthy &amp;&amp; status.CircuitState == CircuitBreakerClosed</span>
}

// GetClusterStatus returns the health status of a specific cluster
func (chm *ClusterHealthMonitor) GetClusterStatus(cluster string) (*ClusterHealthStatus, bool) <span class="cov10" title="2977">{
        chm.mutex.RLock()
        defer chm.mutex.RUnlock()

        status, exists := chm.statuses[cluster]
        if !exists </span><span class="cov1" title="1">{
                return nil, false
        }</span>

        // Return a copy to avoid data races
        <span class="cov9" title="2715">statusCopy := *status
        return &amp;statusCopy, true</span>
}

// GetAllStatuses returns the health status of all monitored clusters
func (chm *ClusterHealthMonitor) GetAllStatuses() map[string]ClusterHealthStatus <span class="cov1" title="1">{
        chm.mutex.RLock()
        defer chm.mutex.RUnlock()

        statuses := make(map[string]ClusterHealthStatus)
        for cluster, status := range chm.statuses </span><span class="cov2" title="3">{
                statuses[cluster] = *status // Copy the status
        }</span>

        <span class="cov1" title="1">return statuses</span>
}

// AddCluster adds a new cluster to monitoring
func (chm *ClusterHealthMonitor) AddCluster(cluster string) <span class="cov5" title="66">{
        chm.mutex.Lock()
        defer chm.mutex.Unlock()

        if _, exists := chm.statuses[cluster]; !exists </span><span class="cov5" title="66">{
                chm.statuses[cluster] = &amp;ClusterHealthStatus{
                        Cluster:      cluster,
                        State:        ClusterHealthUnknown,
                        CircuitState: CircuitBreakerClosed,
                }
                chm.logger.V(2).Info("Added cluster to health monitoring", "cluster", cluster)
        }</span>
}

// RemoveCluster removes a cluster from monitoring
func (chm *ClusterHealthMonitor) RemoveCluster(cluster string) <span class="cov5" title="51">{
        chm.mutex.Lock()
        defer chm.mutex.Unlock()

        if _, exists := chm.statuses[cluster]; exists </span><span class="cov5" title="51">{
                delete(chm.statuses, cluster)
                chm.logger.V(2).Info("Removed cluster from health monitoring", "cluster", cluster)
        }</span>
}

// GetHealthySummary returns a summary of healthy vs unhealthy clusters
func (chm *ClusterHealthMonitor) GetHealthySummary() (healthy, total int) <span class="cov1" title="1">{
        chm.mutex.RLock()
        defer chm.mutex.RUnlock()

        total = len(chm.statuses)
        for _, status := range chm.statuses </span><span class="cov2" title="4">{
                if status.State == ClusterHealthHealthy &amp;&amp; status.CircuitState == CircuitBreakerClosed </span><span class="cov1" title="2">{
                        healthy++
                }</span>
        }

        <span class="cov1" title="1">return healthy, total</span>
}
</pre>
		
		<pre class="file" id="file14" style="display: none">package kubernetes

import "net/http"

// nolint:unused
type impersonateRoundTripper struct {
        delegate http.RoundTripper
}

// nolint:unused
func (irt *impersonateRoundTripper) RoundTrip(req *http.Request) (*http.Response, error) <span class="cov0" title="0">{
        // TODO: Solution won't work with discoveryclient which uses context.TODO() instead of the passed-in context
        if v, ok := req.Context().Value(OAuthAuthorizationHeader).(string); ok </span><span class="cov0" title="0">{
                req.Header.Set("Authorization", v)
        }</span>
        <span class="cov0" title="0">return irt.delegate.RoundTrip(req)</span>
}
</pre>
		
		<pre class="file" id="file15" style="display: none">package kubernetes

import (
        "context"
        "errors"
        "fmt"
        "strings"

        "k8s.io/apimachinery/pkg/runtime"

        "github.com/fsnotify/fsnotify"

        "k8s.io/apimachinery/pkg/api/meta"
        "k8s.io/client-go/discovery"
        "k8s.io/client-go/discovery/cached/memory"
        "k8s.io/client-go/dynamic"
        "k8s.io/client-go/kubernetes/scheme"
        "k8s.io/client-go/rest"
        "k8s.io/client-go/restmapper"
        "k8s.io/client-go/tools/clientcmd"
        clientcmdapi "k8s.io/client-go/tools/clientcmd/api"
        "k8s.io/klog/v2"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config"
        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/helm"

        _ "k8s.io/client-go/plugin/pkg/client/auth/oidc"
)

type HeaderKey string

const (
        CustomAuthorizationHeader = HeaderKey("kubernetes-authorization")
        OAuthAuthorizationHeader  = HeaderKey("Authorization")

        CustomUserAgent = "kubernetes-mcp-server/bearer-token-auth"
)

type CloseWatchKubeConfig func() error

type Kubernetes struct {
        manager             *Manager
        multiClusterManager *MultiClusterManager
}

type Manager struct {
        cfg                     *rest.Config
        clientCmdConfig         clientcmd.ClientConfig
        discoveryClient         discovery.CachedDiscoveryInterface
        accessControlClientSet  *AccessControlClientset
        accessControlRESTMapper *AccessControlRESTMapper
        dynamicClient           *dynamic.DynamicClient

        staticConfig         *config.StaticConfig
        CloseWatchKubeConfig CloseWatchKubeConfig

        // Resource cleanup
        ctx          context.Context
        cancel       context.CancelFunc
        cleanupFuncs []func() error
}

var Scheme = scheme.Scheme
var ParameterCodec = runtime.NewParameterCodec(Scheme)

var _ helm.Kubernetes = &amp;Manager{}

func NewManager(config *config.StaticConfig) (*Manager, error) <span class="cov9" title="23">{
        ctx, cancel := context.WithCancel(context.Background())
        k8s := &amp;Manager{
                staticConfig: config,
                ctx:          ctx,
                cancel:       cancel,
                cleanupFuncs: make([]func() error, 0),
        }
        if err := resolveKubernetesConfigurations(k8s); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        // TODO: Won't work because not all client-go clients use the shared context (e.g. discovery client uses context.TODO())
        //k8s.cfg.Wrap(func(original http.RoundTripper) http.RoundTripper {
        //        return &amp;impersonateRoundTripper{original}
        //})
        <span class="cov9" title="23">var err error
        k8s.accessControlClientSet, err = NewAccessControlClientset(k8s.cfg, k8s.staticConfig)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov9" title="23">k8s.discoveryClient = memory.NewMemCacheClient(k8s.accessControlClientSet.DiscoveryClient())
        k8s.accessControlRESTMapper = NewAccessControlRESTMapper(
                restmapper.NewDeferredDiscoveryRESTMapper(k8s.discoveryClient),
                k8s.staticConfig,
        )
        k8s.dynamicClient, err = dynamic.NewForConfig(k8s.cfg)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov9" title="23">return k8s, nil</span>
}

// NewKubernetes creates a new Kubernetes instance with either single or multi-cluster support
func NewKubernetes(config *config.StaticConfig, logger klog.Logger) (*Kubernetes, error) <span class="cov2" title="2">{
        k8s := &amp;Kubernetes{}

        if config.IsMultiClusterEnabled() </span><span class="cov1" title="1">{
                // Multi-cluster mode
                var err error
                k8s.multiClusterManager, err = NewMultiClusterManager(config, logger)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to create multi-cluster manager: %w", err)
                }</span>

                // Start the multi-cluster manager to discover clusters
                <span class="cov1" title="1">ctx := context.Background()
                if err := k8s.multiClusterManager.Start(ctx); err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to start multi-cluster manager: %w", err)
                }</span>
        } else<span class="cov1" title="1"> {
                // Single-cluster mode (legacy)
                var err error
                k8s.manager, err = NewManager(config)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to create manager: %w", err)
                }</span>
        }

        <span class="cov2" title="2">return k8s, nil</span>
}

// IsMultiCluster returns true if this Kubernetes instance is in multi-cluster mode
func (k *Kubernetes) IsMultiCluster() bool <span class="cov5" title="5">{
        return k.multiClusterManager != nil
}</span>

// GetManager returns the appropriate manager for the current context
func (k *Kubernetes) GetManager() (*Manager, error) <span class="cov0" title="0">{
        if k.IsMultiCluster() </span><span class="cov0" title="0">{
                return k.multiClusterManager.GetActiveManager()
        }</span>
        <span class="cov0" title="0">return k.manager, nil</span>
}

// GetManagerForCluster returns the manager for a specific cluster (multi-cluster mode only)
func (k *Kubernetes) GetManagerForCluster(clusterName string) (*Manager, error) <span class="cov0" title="0">{
        if !k.IsMultiCluster() </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("not in multi-cluster mode")
        }</span>
        <span class="cov0" title="0">return k.multiClusterManager.GetManager(clusterName)</span>
}

// SwitchCluster switches to a different cluster (multi-cluster mode only)
func (k *Kubernetes) SwitchCluster(clusterName string) error <span class="cov0" title="0">{
        if !k.IsMultiCluster() </span><span class="cov0" title="0">{
                return fmt.Errorf("not in multi-cluster mode")
        }</span>
        <span class="cov0" title="0">return k.multiClusterManager.SwitchCluster(clusterName)</span>
}

// GetActiveCluster returns the currently active cluster name (multi-cluster mode only)
func (k *Kubernetes) GetActiveCluster() string <span class="cov0" title="0">{
        if !k.IsMultiCluster() </span><span class="cov0" title="0">{
                return ""
        }</span>
        <span class="cov0" title="0">return k.multiClusterManager.GetActiveCluster()</span>
}

// ListClusters returns all available clusters (multi-cluster mode only)
func (k *Kubernetes) ListClusters() []ClusterConfig <span class="cov1" title="1">{
        if !k.IsMultiCluster() </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov1" title="1">return k.multiClusterManager.ListClusters()</span>
}

// RefreshClusters refreshes the cluster list (multi-cluster mode only)
func (k *Kubernetes) RefreshClusters(ctx context.Context) error <span class="cov0" title="0">{
        if !k.IsMultiCluster() </span><span class="cov0" title="0">{
                return fmt.Errorf("not in multi-cluster mode")
        }</span>
        <span class="cov0" title="0">return k.multiClusterManager.RefreshClusters(ctx)</span>
}

// StartMultiCluster starts the multi-cluster manager (multi-cluster mode only)
func (k *Kubernetes) StartMultiCluster(ctx context.Context) error <span class="cov0" title="0">{
        if !k.IsMultiCluster() </span><span class="cov0" title="0">{
                return fmt.Errorf("not in multi-cluster mode")
        }</span>
        <span class="cov0" title="0">return k.multiClusterManager.Start(ctx)</span>
}

// StopMultiCluster stops the multi-cluster manager (multi-cluster mode only)
func (k *Kubernetes) StopMultiCluster() <span class="cov0" title="0">{
        if k.IsMultiCluster() </span><span class="cov0" title="0">{
                k.multiClusterManager.Stop()
        }</span>
}

// Close properly closes the Kubernetes instance and all its resources
func (k *Kubernetes) Close() <span class="cov2" title="2">{
        if k.IsMultiCluster() </span><span class="cov1" title="1">{
                k.multiClusterManager.Stop()
        }</span> else<span class="cov1" title="1"> if k.manager != nil </span><span class="cov1" title="1">{
                k.manager.Close()
        }</span>
}

func (m *Manager) WatchKubeConfig(onKubeConfigChange func() error) <span class="cov0" title="0">{
        if m.clientCmdConfig == nil </span><span class="cov0" title="0">{
                return
        }</span>
        <span class="cov0" title="0">kubeConfigFiles := m.clientCmdConfig.ConfigAccess().GetLoadingPrecedence()
        if len(kubeConfigFiles) == 0 </span><span class="cov0" title="0">{
                return
        }</span>
        <span class="cov0" title="0">watcher, err := fsnotify.NewWatcher()
        if err != nil </span><span class="cov0" title="0">{
                return
        }</span>
        <span class="cov0" title="0">for _, file := range kubeConfigFiles </span><span class="cov0" title="0">{
                _ = watcher.Add(file)
        }</span>
        <span class="cov0" title="0">go func() </span><span class="cov0" title="0">{
                defer watcher.Close()
                for </span><span class="cov0" title="0">{
                        select </span>{
                        case &lt;-m.ctx.Done():<span class="cov0" title="0">
                                return</span>
                        case _, ok := &lt;-watcher.Events:<span class="cov0" title="0">
                                if !ok </span><span class="cov0" title="0">{
                                        return
                                }</span>
                                <span class="cov0" title="0">_ = onKubeConfigChange()</span>
                        case _, ok := &lt;-watcher.Errors:<span class="cov0" title="0">
                                if !ok </span><span class="cov0" title="0">{
                                        return
                                }</span>
                        }
                }
        }()
        <span class="cov0" title="0">if m.CloseWatchKubeConfig != nil </span><span class="cov0" title="0">{
                _ = m.CloseWatchKubeConfig()
        }</span>
        <span class="cov0" title="0">m.CloseWatchKubeConfig = watcher.Close</span>
}

func (m *Manager) Close() <span class="cov10" title="25">{
        // Cancel context to stop all goroutines
        if m.cancel != nil </span><span class="cov10" title="25">{
                m.cancel()
        }</span>

        // Run all cleanup functions
        <span class="cov10" title="25">for _, cleanup := range m.cleanupFuncs </span><span class="cov7" title="12">{
                if err := cleanup(); err != nil </span><span class="cov2" title="2">{
                        // Log error but continue with other cleanups
                        klog.Errorf("Error during cleanup: %v", err)
                }</span>
        }

        // Close kubeconfig watcher
        <span class="cov10" title="25">if m.CloseWatchKubeConfig != nil </span><span class="cov0" title="0">{
                _ = m.CloseWatchKubeConfig()
        }</span>

        // Clear discovery cache to free memory
        <span class="cov10" title="25">if m.discoveryClient != nil </span><span class="cov10" title="25">{
                m.discoveryClient.Invalidate()
        }</span>
}

// RegisterCleanup registers a cleanup function to be called when the manager is closed
func (m *Manager) RegisterCleanup(cleanup func() error) <span class="cov7" title="10">{
        m.cleanupFuncs = append(m.cleanupFuncs, cleanup)
}</span>

// GetContext returns the manager's context for long-running operations
func (m *Manager) GetContext() context.Context <span class="cov1" title="1">{
        return m.ctx
}</span>

func (m *Manager) GetAPIServerHost() string <span class="cov0" title="0">{
        if m.cfg == nil </span><span class="cov0" title="0">{
                return ""
        }</span>
        <span class="cov0" title="0">return m.cfg.Host</span>
}

func (m *Manager) ToDiscoveryClient() (discovery.CachedDiscoveryInterface, error) <span class="cov0" title="0">{
        return m.discoveryClient, nil
}</span>

func (m *Manager) ToRESTMapper() (meta.RESTMapper, error) <span class="cov0" title="0">{
        return m.accessControlRESTMapper, nil
}</span>

func (m *Manager) Derived(ctx context.Context) (*Kubernetes, error) <span class="cov6" title="6">{
        authorization, ok := ctx.Value(OAuthAuthorizationHeader).(string)
        if !ok || !strings.HasPrefix(authorization, "Bearer ") </span><span class="cov4" title="4">{
                if m.staticConfig.RequireOAuth </span><span class="cov2" title="2">{
                        return nil, errors.New("oauth token required")
                }</span>
                <span class="cov2" title="2">return &amp;Kubernetes{manager: m}, nil</span>
        }
        <span class="cov2" title="2">klog.V(5).Infof("%s header found (Bearer), using provided bearer token", OAuthAuthorizationHeader)
        derivedCfg := &amp;rest.Config{
                Host:    m.cfg.Host,
                APIPath: m.cfg.APIPath,
                // Copy only server verification TLS settings (CA bundle and server name)
                TLSClientConfig: rest.TLSClientConfig{
                        Insecure:   m.cfg.Insecure,
                        ServerName: m.cfg.ServerName,
                        CAFile:     m.cfg.CAFile,
                        CAData:     m.cfg.CAData,
                },
                BearerToken: strings.TrimPrefix(authorization, "Bearer "),
                // pass custom UserAgent to identify the client
                UserAgent:   CustomUserAgent,
                QPS:         m.cfg.QPS,
                Burst:       m.cfg.Burst,
                Timeout:     m.cfg.Timeout,
                Impersonate: rest.ImpersonationConfig{},
        }
        clientCmdApiConfig, err := m.clientCmdConfig.RawConfig()
        if err != nil </span><span class="cov0" title="0">{
                if m.staticConfig.RequireOAuth </span><span class="cov0" title="0">{
                        klog.Errorf("failed to get kubeconfig: %v", err)
                        return nil, errors.New("failed to get kubeconfig")
                }</span>
                <span class="cov0" title="0">return &amp;Kubernetes{manager: m}, nil</span>
        }
        <span class="cov2" title="2">clientCmdApiConfig.AuthInfos = make(map[string]*clientcmdapi.AuthInfo)
        derived := &amp;Kubernetes{manager: &amp;Manager{
                clientCmdConfig: clientcmd.NewDefaultClientConfig(clientCmdApiConfig, nil),
                cfg:             derivedCfg,
                staticConfig:    m.staticConfig,
        }}
        derived.manager.accessControlClientSet, err = NewAccessControlClientset(derived.manager.cfg, derived.manager.staticConfig)
        if err != nil </span><span class="cov0" title="0">{
                if m.staticConfig.RequireOAuth </span><span class="cov0" title="0">{
                        klog.Errorf("failed to get kubeconfig: %v", err)
                        return nil, errors.New("failed to get kubeconfig")
                }</span>
                <span class="cov0" title="0">return &amp;Kubernetes{manager: m}, nil</span>
        }
        <span class="cov2" title="2">derived.manager.discoveryClient = memory.NewMemCacheClient(derived.manager.accessControlClientSet.DiscoveryClient())
        derived.manager.accessControlRESTMapper = NewAccessControlRESTMapper(
                restmapper.NewDeferredDiscoveryRESTMapper(derived.manager.discoveryClient),
                derived.manager.staticConfig,
        )
        derived.manager.dynamicClient, err = dynamic.NewForConfig(derived.manager.cfg)
        if err != nil </span><span class="cov0" title="0">{
                if m.staticConfig.RequireOAuth </span><span class="cov0" title="0">{
                        klog.Errorf("failed to initialize dynamic client: %v", err)
                        return nil, errors.New("failed to initialize dynamic client")
                }</span>
                <span class="cov0" title="0">return &amp;Kubernetes{manager: m}, nil</span>
        }
        <span class="cov2" title="2">return derived, nil</span>
}

func (k *Kubernetes) NewHelm() *helm.Helm <span class="cov0" title="0">{
        // This is a derived Kubernetes, so it already has the Helm initialized
        return helm.NewHelm(k.manager)
}</span>
</pre>
		
		<pre class="file" id="file16" style="display: none">package kubernetes

import (
        "context"
        "fmt"
        "os"
        "path/filepath"
        "strings"
        "sync"
        "time"

        "k8s.io/klog/v2"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config"
        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/nsk"
)

// MultiClusterManager extends the existing Manager to support multiple clusters
type MultiClusterManager struct {
        staticConfig  *config.StaticConfig
        clusters      map[string]*Manager
        activeCluster string
        nskManager    *nsk.Manager
        healthMonitor *ClusterHealthMonitor
        logger        klog.Logger
        mutex         sync.RWMutex
}

// ClusterConfig represents the configuration for a single cluster
type ClusterConfig struct {
        Name         string    `json:"name"`
        KubeConfig   string    `json:"kubeconfig_path"`
        IsActive     bool      `json:"is_active"`
        LastAccessed time.Time `json:"last_accessed"`
        Environment  string    `json:"environment,omitempty"`
        Description  string    `json:"description,omitempty"`
}

// NewMultiClusterManager creates a new multi-cluster manager
func NewMultiClusterManager(staticConfig *config.StaticConfig, logger klog.Logger) (*MultiClusterManager, error) <span class="cov2" title="4">{
        if !staticConfig.IsMultiClusterEnabled() </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("multi-cluster mode not enabled")
        }</span>

        <span class="cov2" title="4">mcm := &amp;MultiClusterManager{
                staticConfig: staticConfig,
                clusters:     make(map[string]*Manager),
                logger:       logger,
        }

        // Initialize NSK manager if NSK integration is enabled
        if staticConfig.IsNSKEnabled() </span><span class="cov0" title="0">{
                var err error
                mcm.nskManager, err = nsk.NewManager(staticConfig.NSKIntegration, logger)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to create NSK manager: %w", err)
                }</span>
        }

        // Initialize health monitor
        <span class="cov2" title="4">healthConfig := DefaultClusterHealthConfig()
        mcm.healthMonitor = NewClusterHealthMonitor(healthConfig, logger, mcm.performHealthCheck)

        return mcm, nil</span>
}

// Start initializes the multi-cluster manager and discovers clusters
func (mcm *MultiClusterManager) Start(ctx context.Context) error <span class="cov2" title="4">{
        mcm.logger.Info("Starting multi-cluster manager")

        // Start NSK manager if enabled
        if mcm.nskManager != nil </span><span class="cov0" title="0">{
                if err := mcm.nskManager.Start(ctx); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to start NSK manager: %w", err)
                }</span>
        }

        // Discover and initialize clusters
        <span class="cov2" title="4">if err := mcm.DiscoverClusters(ctx); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("cluster discovery failed: %w", err)
        }</span>

        // Start health monitoring for discovered clusters
        <span class="cov2" title="4">clusterNames := make([]string, 0, len(mcm.clusters))
        for name := range mcm.clusters </span><span class="cov3" title="8">{
                clusterNames = append(clusterNames, name)
        }</span>
        <span class="cov2" title="4">mcm.healthMonitor.Start(ctx, clusterNames)

        // Set default cluster if specified
        if mcm.staticConfig.DefaultCluster != "" </span><span class="cov0" title="0">{
                if err := mcm.SwitchCluster(mcm.staticConfig.DefaultCluster); err != nil </span><span class="cov0" title="0">{
                        mcm.logger.Error(err, "Failed to set default cluster", "cluster", mcm.staticConfig.DefaultCluster)
                }</span>
        } else<span class="cov2" title="4"> if len(mcm.clusters) &gt; 0 </span><span class="cov2" title="4">{
                // Auto-select first cluster if no default specified
                for name := range mcm.clusters </span><span class="cov2" title="4">{
                        mcm.activeCluster = name
                        break</span>
                }
        }

        <span class="cov2" title="4">mcm.logger.Info("Multi-cluster manager started", "clusters", len(mcm.clusters), "active", mcm.activeCluster)
        return nil</span>
}

// Stop stops the multi-cluster manager
func (mcm *MultiClusterManager) Stop() <span class="cov4" title="13">{
        mcm.logger.Info("Stopping multi-cluster manager")

        if mcm.healthMonitor != nil </span><span class="cov4" title="13">{
                mcm.healthMonitor.Stop()
        }</span>

        <span class="cov4" title="13">if mcm.nskManager != nil </span><span class="cov0" title="0">{
                mcm.nskManager.Stop()
        }</span>

        // Clean up all cluster managers
        <span class="cov4" title="13">mcm.mutex.Lock()
        for name, manager := range mcm.clusters </span><span class="cov3" title="8">{
                mcm.logger.V(2).Info("Closing cluster manager", "cluster", name)
                manager.Close()
        }</span>
        <span class="cov4" title="13">mcm.clusters = make(map[string]*Manager)
        mcm.mutex.Unlock()

        mcm.logger.Info("Multi-cluster manager stopped")</span>
}

// DiscoverClusters discovers available clusters from kubeconfig directory or NSK
func (mcm *MultiClusterManager) DiscoverClusters(ctx context.Context) error <span class="cov3" title="5">{
        mcm.logger.V(2).Info("Discovering clusters")

        var discoveredClusters map[string]string // name -&gt; kubeconfig path

        if mcm.nskManager != nil </span><span class="cov0" title="0">{
                // Use NSK to discover clusters
                nskClusters := mcm.nskManager.GetClusters()
                discoveredClusters = make(map[string]string)
                for name, cluster := range nskClusters </span><span class="cov0" title="0">{
                        discoveredClusters[name] = cluster.KubeConfig
                }</span>
        } else<span class="cov3" title="5"> {
                // Scan kubeconfig directory
                var err error
                discoveredClusters, err = mcm.scanKubeConfigDirectory()
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to scan kubeconfig directory: %w", err)
                }</span>
        }

        // Apply cluster aliases
        <span class="cov3" title="5">discoveredClusters = mcm.applyClusterAliases(discoveredClusters)

        // Initialize Manager instances for each cluster
        newClusters := make(map[string]*Manager)
        var failedClusters []string
        for name, kubeconfigPath := range discoveredClusters </span><span class="cov3" title="10">{
                mcm.logger.V(2).Info("Creating manager for cluster", "cluster", name, "kubeconfig", kubeconfigPath)
                manager, err := mcm.createClusterManager(name, kubeconfigPath)
                if err != nil </span><span class="cov0" title="0">{
                        mcm.logger.Error(err, "Failed to create manager for cluster", "cluster", name, "kubeconfig", kubeconfigPath)
                        failedClusters = append(failedClusters, name)
                        continue</span>
                }
                <span class="cov3" title="10">newClusters[name] = manager
                mcm.logger.V(2).Info("Successfully created manager for cluster", "cluster", name)</span>
        }

        <span class="cov3" title="5">if len(failedClusters) &gt; 0 </span><span class="cov0" title="0">{
                mcm.logger.Info("Some clusters failed to initialize", "failed_clusters", failedClusters, "successful_clusters", len(newClusters), "total_discovered", len(discoveredClusters))
        }</span>

        <span class="cov3" title="5">mcm.mutex.Lock()
        oldClusters := mcm.clusters
        mcm.clusters = newClusters
        mcm.mutex.Unlock()

        // Clean up removed clusters and update health monitoring
        if mcm.healthMonitor != nil </span><span class="cov2" title="4">{
                // Remove clusters that are no longer available
                for oldCluster, oldManager := range oldClusters </span><span class="cov0" title="0">{
                        if _, exists := newClusters[oldCluster]; !exists </span><span class="cov0" title="0">{
                                mcm.logger.V(2).Info("Cleaning up removed cluster", "cluster", oldCluster)
                                mcm.healthMonitor.RemoveCluster(oldCluster)
                                oldManager.Close()
                        }</span>
                }

                // Add new clusters to monitoring
                <span class="cov2" title="4">for newCluster := range newClusters </span><span class="cov3" title="8">{
                        if _, exists := oldClusters[newCluster]; !exists </span><span class="cov3" title="8">{
                                mcm.healthMonitor.AddCluster(newCluster)
                        }</span>
                }
        } else<span class="cov1" title="1"> {
                // Just clean up removed clusters if no health monitoring
                for oldCluster, oldManager := range oldClusters </span><span class="cov0" title="0">{
                        if _, exists := newClusters[oldCluster]; !exists </span><span class="cov0" title="0">{
                                mcm.logger.V(2).Info("Cleaning up removed cluster", "cluster", oldCluster)
                                oldManager.Close()
                        }</span>
                }
        }

        <span class="cov3" title="5">mcm.logger.V(2).Info("Cluster discovery completed", "count", len(newClusters))
        return nil</span>
}

// scanKubeConfigDirectory scans the kubeconfig directory for cluster files
func (mcm *MultiClusterManager) scanKubeConfigDirectory() (map[string]string, error) <span class="cov3" title="10">{
        clusters := make(map[string]string)

        if mcm.staticConfig.KubeConfigDir == "" </span><span class="cov1" title="1">{
                mcm.logger.V(2).Info("No kubeconfig directory specified")
                return clusters, nil
        }</span>

        <span class="cov3" title="9">mcm.logger.V(2).Info("Scanning kubeconfig directory", "directory", mcm.staticConfig.KubeConfigDir)

        err := filepath.Walk(mcm.staticConfig.KubeConfigDir, func(path string, info os.FileInfo, err error) error </span><span class="cov5" title="31">{
                if err != nil </span><span class="cov1" title="1">{
                        mcm.logger.Error(err, "Error walking directory", "path", path)
                        return err
                }</span>

                <span class="cov5" title="30">if info.IsDir() </span><span class="cov3" title="9">{
                        mcm.logger.V(3).Info("Skipping directory", "path", path)
                        return nil
                }</span>

                <span class="cov4" title="21">mcm.logger.V(3).Info("Found file", "path", path, "name", info.Name())

                // Check if file is a YAML kubeconfig file
                if strings.HasSuffix(path, ".yaml") || strings.HasSuffix(path, ".yml") </span><span class="cov4" title="19">{
                        // Extract cluster name from filename
                        basename := filepath.Base(path)
                        clusterName := strings.TrimSuffix(basename, filepath.Ext(basename))
                        clusters[clusterName] = path
                        mcm.logger.V(2).Info("Discovered cluster config", "cluster", clusterName, "path", path)
                }</span> else<span class="cov1" title="2"> {
                        mcm.logger.V(3).Info("Skipping non-YAML file", "path", path)
                }</span>

                <span class="cov4" title="21">return nil</span>
        })

        <span class="cov3" title="9">mcm.logger.V(2).Info("Directory scan completed", "clusters_found", len(clusters))
        for name, path := range clusters </span><span class="cov4" title="19">{
                mcm.logger.V(2).Info("Cluster discovered", "name", name, "path", path)
        }</span>

        <span class="cov3" title="9">return clusters, err</span>
}

// applyClusterAliases applies cluster aliases from configuration
func (mcm *MultiClusterManager) applyClusterAliases(clusters map[string]string) map[string]string <span class="cov3" title="6">{
        if len(mcm.staticConfig.ClusterAliases) == 0 </span><span class="cov3" title="5">{
                return clusters
        }</span>

        <span class="cov1" title="1">aliased := make(map[string]string)

        // First, add all original clusters
        for name, path := range clusters </span><span class="cov2" title="3">{
                aliased[name] = path
        }</span>

        // Then add aliases
        <span class="cov1" title="1">for alias, target := range mcm.staticConfig.ClusterAliases </span><span class="cov2" title="3">{
                if path, exists := clusters[target]; exists </span><span class="cov1" title="2">{
                        aliased[alias] = path
                }</span>
        }

        <span class="cov1" title="1">return aliased</span>
}

// createClusterManager creates a Manager instance for a specific cluster
func (mcm *MultiClusterManager) createClusterManager(clusterName, kubeconfigPath string) (*Manager, error) <span class="cov3" title="10">{
        mcm.logger.V(2).Info("Creating manager for cluster", "cluster", clusterName, "kubeconfig", kubeconfigPath)

        // Create a copy of the static config with the specific kubeconfig
        clusterConfig := *mcm.staticConfig
        clusterConfig.KubeConfig = kubeconfigPath
        clusterConfig.KubeConfigDir = "" // Clear multi-cluster config to use single kubeconfig

        // Force fresh client creation by clearing any cached configurations
        // This ensures we don't inherit stale authentication state

        mcm.logger.V(3).Info("Creating manager with config", "cluster", clusterName,
                "kubeconfig", clusterConfig.KubeConfig)

        // Create manager for this cluster
        manager, err := NewManager(&amp;clusterConfig)
        if err != nil </span><span class="cov0" title="0">{
                mcm.logger.Error(err, "Failed to create manager", "cluster", clusterName, "kubeconfig", kubeconfigPath)
                return nil, fmt.Errorf("failed to create manager for cluster %s: %w", clusterName, err)
        }</span>

        // Validate the manager has proper configuration
        <span class="cov3" title="10">restConfig, err := manager.ToRESTConfig()
        if err != nil || restConfig == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("cluster %s manager has invalid rest config: %w", clusterName, err)
        }</span>

        <span class="cov3" title="10">mcm.logger.V(2).Info("Successfully created manager for cluster", "cluster", clusterName,
                "server", restConfig.Host)

        return manager, nil</span>
}

// SwitchCluster switches to a different cluster
func (mcm *MultiClusterManager) SwitchCluster(clusterName string) error <span class="cov10" title="1005">{
        mcm.mutex.Lock()
        defer mcm.mutex.Unlock()

        // Check if cluster exists
        manager, exists := mcm.clusters[clusterName]
        if !exists </span><span class="cov1" title="1">{
                availableClusters := make([]string, 0, len(mcm.clusters))
                for name := range mcm.clusters </span><span class="cov1" title="2">{
                        availableClusters = append(availableClusters, name)
                }</span>
                <span class="cov1" title="1">return fmt.Errorf("cluster %s not found, available clusters: %v", clusterName, availableClusters)</span>
        }

        // Validate the manager is properly configured
        <span class="cov9" title="1004">if manager == nil </span><span class="cov1" title="1">{
                return fmt.Errorf("cluster %s has no manager instance", clusterName)
        }</span>
        <span class="cov9" title="1003">if manager.staticConfig == nil </span><span class="cov1" title="1">{
                return fmt.Errorf("cluster %s manager has no configuration", clusterName)
        }</span>
        <span class="cov9" title="1002">if manager.staticConfig.KubeConfig == "" </span><span class="cov1" title="1">{
                return fmt.Errorf("cluster %s manager has no kubeconfig path", clusterName)
        }</span>

        // Store previous cluster for logging
        <span class="cov9" title="1001">previousCluster := mcm.activeCluster

        mcm.logger.V(2).Info("Attempting cluster switch",
                "from", previousCluster,
                "to", clusterName,
                "kubeconfig", manager.staticConfig.KubeConfig,
                "server", func() string </span><span class="cov9" title="1001">{
                        if restConfig, err := manager.ToRESTConfig(); err == nil &amp;&amp; restConfig != nil </span><span class="cov0" title="0">{
                                return restConfig.Host
                        }</span>
                        <span class="cov9" title="1001">return "unknown"</span>
                }())

        // Switch to the new cluster - each manager is pre-configured with its own kubeconfig
        <span class="cov9" title="1001">mcm.activeCluster = clusterName

        // Note: We skip synchronous health check here to avoid blocking the response.
        // The background health monitor will check cluster health asynchronously.

        mcm.logger.Info("Switched cluster",
                "from", previousCluster,
                "to", clusterName,
                "kubeconfig", manager.staticConfig.KubeConfig)

        return nil</span>
}

// GetActiveCluster returns the currently active cluster
func (mcm *MultiClusterManager) GetActiveCluster() string <span class="cov2" title="3">{
        mcm.mutex.RLock()
        defer mcm.mutex.RUnlock()
        return mcm.activeCluster
}</span>

// GetActiveManager returns the Manager for the currently active cluster
func (mcm *MultiClusterManager) GetActiveManager() (*Manager, error) <span class="cov9" title="1004">{
        mcm.mutex.RLock()
        defer mcm.mutex.RUnlock()

        if mcm.activeCluster == "" </span><span class="cov1" title="1">{
                return nil, fmt.Errorf("no active cluster")
        }</span>

        <span class="cov9" title="1003">manager, exists := mcm.clusters[mcm.activeCluster]
        if !exists </span><span class="cov1" title="1">{
                return nil, fmt.Errorf("active cluster %s not found", mcm.activeCluster)
        }</span>

        <span class="cov9" title="1002">return manager, nil</span>
}

// GetManager returns the Manager for a specific cluster
func (mcm *MultiClusterManager) GetManager(clusterName string) (*Manager, error) <span class="cov0" title="0">{
        mcm.mutex.RLock()
        defer mcm.mutex.RUnlock()

        manager, exists := mcm.clusters[clusterName]
        if !exists </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("cluster %s not found", clusterName)
        }</span>

        <span class="cov0" title="0">return manager, nil</span>
}

// ListClusters returns information about all available clusters
func (mcm *MultiClusterManager) ListClusters() []ClusterConfig <span class="cov1" title="1">{
        mcm.mutex.RLock()
        defer mcm.mutex.RUnlock()

        clusters := make([]ClusterConfig, 0, len(mcm.clusters))

        for name, manager := range mcm.clusters </span><span class="cov0" title="0">{
                config := ClusterConfig{
                        Name:       name,
                        KubeConfig: manager.staticConfig.KubeConfig,
                        IsActive:   name == mcm.activeCluster,
                }

                // Get additional info from NSK if available
                if mcm.nskManager != nil </span><span class="cov0" title="0">{
                        if nskCluster, err := mcm.nskManager.GetCluster(name); err == nil </span><span class="cov0" title="0">{
                                config.Environment = nskCluster.Environment
                                config.Description = nskCluster.Description
                                config.LastAccessed = nskCluster.LastRefresh
                        }</span>
                }

                <span class="cov0" title="0">clusters = append(clusters, config)</span>
        }

        <span class="cov1" title="1">return clusters</span>
}

// ValidateCluster checks if a cluster is accessible
func (mcm *MultiClusterManager) ValidateCluster(clusterName string) error <span class="cov0" title="0">{
        manager, err := mcm.GetManager(clusterName)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Basic validation - check if config is valid
        <span class="cov0" title="0">if manager.cfg == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("cluster %s has invalid configuration", clusterName)
        }</span>

        // Additional validation via NSK if available
        <span class="cov0" title="0">if mcm.nskManager != nil </span><span class="cov0" title="0">{
                return mcm.nskManager.ValidateCluster(clusterName)
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// RefreshClusters refreshes the cluster list
func (mcm *MultiClusterManager) RefreshClusters(ctx context.Context) error <span class="cov0" title="0">{
        mcm.logger.V(2).Info("Refreshing clusters")

        // Refresh NSK clusters if NSK is enabled
        if mcm.nskManager != nil </span><span class="cov0" title="0">{
                if err := mcm.nskManager.RefreshClusters(ctx); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("NSK cluster refresh failed: %w", err)
                }</span>
        }

        // Rediscover clusters
        <span class="cov0" title="0">return mcm.DiscoverClusters(ctx)</span>
}

// GetClusterCount returns the number of available clusters
func (mcm *MultiClusterManager) GetClusterCount() int <span class="cov1" title="2">{
        mcm.mutex.RLock()
        defer mcm.mutex.RUnlock()
        return len(mcm.clusters)
}</span>

// IsNSKEnabled returns true if NSK integration is enabled
func (mcm *MultiClusterManager) IsNSKEnabled() bool <span class="cov0" title="0">{
        return mcm.nskManager != nil
}</span>

// GetNSKStatus returns the NSK manager status if NSK is enabled
func (mcm *MultiClusterManager) GetNSKStatus() *nsk.ManagerStatus <span class="cov0" title="0">{
        if mcm.nskManager == nil </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">return mcm.nskManager.GetStatus()</span>
}

// performHealthCheck performs a health check on a specific cluster
func (mcm *MultiClusterManager) performHealthCheck(ctx context.Context, cluster string) error <span class="cov0" title="0">{
        mcm.logger.V(3).Info("Performing health check", "cluster", cluster)

        manager, err := mcm.GetManager(cluster)
        if err != nil </span><span class="cov0" title="0">{
                mcm.logger.V(3).Info("Failed to get manager for health check", "cluster", cluster, "error", err)
                return fmt.Errorf("failed to get manager for cluster %s: %w", cluster, err)
        }</span>

        // Validate manager configuration
        <span class="cov0" title="0">restConfig, err := manager.ToRESTConfig()
        if err != nil || restConfig == nil </span><span class="cov0" title="0">{
                mcm.logger.V(3).Info("Manager has no rest config", "cluster", cluster, "error", err)
                return fmt.Errorf("cluster %s manager has no rest config: %w", cluster, err)
        }</span>

        // Try to get the discovery client and perform a simple API call
        <span class="cov0" title="0">client, err := manager.ToDiscoveryClient()
        if err != nil </span><span class="cov0" title="0">{
                mcm.logger.V(3).Info("Failed to get discovery client", "cluster", cluster, "error", err)
                return fmt.Errorf("failed to get discovery client for cluster %s: %w", cluster, err)
        }</span>

        // Perform a simple API call to check cluster health and authentication
        <span class="cov0" title="0">serverVersion, err := client.ServerVersion()
        if err != nil </span><span class="cov0" title="0">{
                mcm.logger.V(3).Info("Health check failed", "cluster", cluster, "error", err, "server", restConfig.Host)

                // Provide more detailed error messages for common authentication issues
                if strings.Contains(err.Error(), "credentials") || strings.Contains(err.Error(), "Unauthorized") </span><span class="cov0" title="0">{
                        return fmt.Errorf("cluster %s authentication failed - server requested credentials: %w", cluster, err)
                }</span>
                <span class="cov0" title="0">return fmt.Errorf("cluster %s health check failed: %w", cluster, err)</span>
        }

        <span class="cov0" title="0">mcm.logger.V(3).Info("Health check successful", "cluster", cluster,
                "server", restConfig.Host, "version", serverVersion.String())

        return nil</span>
}

// IsClusterHealthy returns true if the cluster is healthy
func (mcm *MultiClusterManager) IsClusterHealthy(cluster string) bool <span class="cov0" title="0">{
        if mcm.healthMonitor == nil </span><span class="cov0" title="0">{
                return true // Assume healthy if monitoring is disabled
        }</span>
        <span class="cov0" title="0">return mcm.healthMonitor.IsHealthy(cluster)</span>
}

// GetClusterHealth returns the health status of a specific cluster
func (mcm *MultiClusterManager) GetClusterHealth(cluster string) (*ClusterHealthStatus, bool) <span class="cov0" title="0">{
        if mcm.healthMonitor == nil </span><span class="cov0" title="0">{
                return nil, false
        }</span>
        <span class="cov0" title="0">return mcm.healthMonitor.GetClusterStatus(cluster)</span>
}

// GetAllClusterHealth returns the health status of all clusters
func (mcm *MultiClusterManager) GetAllClusterHealth() map[string]ClusterHealthStatus <span class="cov0" title="0">{
        if mcm.healthMonitor == nil </span><span class="cov0" title="0">{
                return make(map[string]ClusterHealthStatus)
        }</span>
        <span class="cov0" title="0">return mcm.healthMonitor.GetAllStatuses()</span>
}

// GetHealthySummary returns a summary of healthy vs total clusters
func (mcm *MultiClusterManager) GetHealthySummary() (healthy, total int) <span class="cov0" title="0">{
        if mcm.healthMonitor == nil </span><span class="cov0" title="0">{
                return len(mcm.clusters), len(mcm.clusters)
        }</span>
        <span class="cov0" title="0">return mcm.healthMonitor.GetHealthySummary()</span>
}
</pre>
		
		<pre class="file" id="file17" style="display: none">package kubernetes

import (
        "context"
        "k8s.io/apimachinery/pkg/runtime"
        "k8s.io/apimachinery/pkg/runtime/schema"
)

func (k *Kubernetes) NamespacesList(ctx context.Context, options ResourceListOptions) (runtime.Unstructured, error) <span class="cov0" title="0">{
        return k.ResourcesList(ctx, &amp;schema.GroupVersionKind{
                Group: "", Version: "v1", Kind: "Namespace",
        }, "", options)
}</span>

func (k *Kubernetes) ProjectsList(ctx context.Context, options ResourceListOptions) (runtime.Unstructured, error) <span class="cov0" title="0">{
        return k.ResourcesList(ctx, &amp;schema.GroupVersionKind{
                Group: "project.openshift.io", Version: "v1", Kind: "Project",
        }, "", options)
}</span>
</pre>
		
		<pre class="file" id="file18" style="display: none">package kubernetes

import (
        "context"
        "fmt"
        "os"
        "os/exec"
        "strings"
        "sync"
        "time"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config"
        "k8s.io/klog/v2"
)

// NSKIntegration manages NSK (Netskope Kubernetes) tool integration
type NSKIntegration struct {
        config         *config.NSKConfig
        clusterManager *MultiClusterManager
        refreshTicker  *time.Ticker
        stopChan       chan struct{}
        logger         klog.Logger
        lastRefresh    time.Time
        nextRefresh    time.Time
        refreshHistory []RefreshResult
        mu             sync.RWMutex
}

// RefreshResult represents the result of a refresh operation
type RefreshResult struct {
        Timestamp time.Time
        Success   bool
        Error     error
        Clusters  []string
        Details   string
}

// NewNSKIntegrationForMCM creates a new NSK integration for MultiClusterManager
func NewNSKIntegrationForMCM(cfg *config.NSKConfig, mcm *MultiClusterManager) *NSKIntegration <span class="cov6" title="16">{
        return &amp;NSKIntegration{
                config:         cfg,
                clusterManager: mcm,
                stopChan:       make(chan struct{}),
                logger:         klog.Background(),
                refreshHistory: make([]RefreshResult, 0, 100),
        }
}</span>

// Start initializes the NSK integration and starts auto-refresh if enabled
func (nsk *NSKIntegration) Start(ctx context.Context) error <span class="cov1" title="1">{
        // Set environment variables
        if err := nsk.setEnvironment(); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to set NSK environment: %w", err)
        }</span>

        // Ensure config directory exists
        <span class="cov1" title="1">if nsk.config.ConfigDir != "" </span><span class="cov1" title="1">{
                if err := os.MkdirAll(nsk.config.ConfigDir, 0700); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to create config directory: %w", err)
                }</span>
        }

        // Initial refresh
        <span class="cov1" title="1">if err := nsk.RefreshKubeConfigs(ctx); err != nil </span><span class="cov1" title="1">{
                nsk.logger.Error(err, "Initial kubeconfig refresh failed")
                // Don't fail startup if initial refresh fails
        }</span>

        // Start auto-refresh if enabled
        <span class="cov1" title="1">if nsk.config.AutoRefresh </span><span class="cov0" title="0">{
                interval, err := time.ParseDuration(nsk.config.RefreshInterval)
                if err != nil </span><span class="cov0" title="0">{
                        interval = time.Hour // default to 1 hour
                        nsk.logger.Info("Invalid refresh interval, using default", "interval", interval)
                }</span>

                <span class="cov0" title="0">nsk.refreshTicker = time.NewTicker(interval)
                go nsk.refreshLoop(ctx)

                nsk.mu.Lock()
                nsk.nextRefresh = time.Now().Add(interval)
                nsk.mu.Unlock()</span>
        }

        <span class="cov1" title="1">return nil</span>
}

// Stop stops the NSK integration and cleanup resources
func (nsk *NSKIntegration) Stop() <span class="cov1" title="1">{
        if nsk.refreshTicker != nil </span><span class="cov0" title="0">{
                nsk.refreshTicker.Stop()
        }</span>
        <span class="cov1" title="1">close(nsk.stopChan)</span>
}

// setEnvironment sets NSK environment variables
func (nsk *NSKIntegration) setEnvironment() error <span class="cov2" title="2">{
        envVars := map[string]string{}

        // Set from config
        if nsk.config.RancherURL != "" </span><span class="cov2" title="2">{
                envVars["RANCHER_URL"] = nsk.config.RancherURL
        }</span>
        <span class="cov2" title="2">if nsk.config.RancherToken != "" </span><span class="cov2" title="2">{
                envVars["RANCHER_TOKEN"] = nsk.config.RancherToken
        }</span>
        <span class="cov2" title="2">if nsk.config.Profile != "" </span><span class="cov2" title="2">{
                envVars["NSK_PROFILE"] = nsk.config.Profile
        }</span>
        <span class="cov2" title="2">if nsk.config.ConfigDir != "" </span><span class="cov2" title="2">{
                envVars["NSK_CONFDIR"] = nsk.config.ConfigDir
        }</span>

        // Add custom environment variables
        <span class="cov2" title="2">for key, value := range nsk.config.Environment </span><span class="cov1" title="1">{
                envVars[key] = value
        }</span>

        // Set environment variables
        <span class="cov2" title="2">for key, value := range envVars </span><span class="cov5" title="9">{
                if value != "" </span><span class="cov5" title="9">{
                        if err := os.Setenv(key, value); err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed to set %s: %w", key, err)
                        }</span>
                }
        }

        // Log environment setup (without sensitive values)
        <span class="cov2" title="2">nsk.logger.Info("NSK environment configured",
                "rancher_url", nsk.config.RancherURL,
                "profile", nsk.config.Profile,
                "config_dir", nsk.config.ConfigDir,
                "token_set", nsk.config.RancherToken != "")

        return nil</span>
}

// RefreshKubeConfigs refreshes all kubeconfig files from Rancher via NSK
func (nsk *NSKIntegration) RefreshKubeConfigs(ctx context.Context) error <span class="cov1" title="1">{
        nsk.logger.Info("Refreshing kubeconfigs from Rancher via NSK")

        startTime := time.Now()
        result := RefreshResult{
                Timestamp: startTime,
                Success:   false,
        }

        // Build NSK command
        args := []string{"cluster", "kubeconfig"}

        // Add cluster pattern if specified
        if nsk.config.ClusterPattern != "" </span><span class="cov0" title="0">{
                args = append(args, "--name-pattern", nsk.config.ClusterPattern)
        }</span>

        // Execute NSK command
        <span class="cov1" title="1">cmd := exec.CommandContext(ctx, nsk.getNSKPath(), args...)
        cmd.Dir = nsk.config.ConfigDir

        // Capture environment for the command
        cmd.Env = os.Environ()

        output, err := cmd.CombinedOutput()
        if err != nil </span><span class="cov1" title="1">{
                result.Error = fmt.Errorf("NSK kubeconfig refresh failed: %w, output: %s", err, output)
                nsk.addRefreshResult(result)
                return result.Error
        }</span>

        <span class="cov0" title="0">nsk.logger.Info("NSK kubeconfig refresh completed", "output", string(output))

        // Parse output to get list of clusters
        result.Clusters = nsk.parseNSKOutput(string(output))
        result.Details = string(output)

        // Trigger cluster manager to rescan directory
        if err := nsk.clusterManager.RefreshClusters(ctx); err != nil </span><span class="cov0" title="0">{
                result.Error = fmt.Errorf("failed to rediscover clusters: %w", err)
                nsk.addRefreshResult(result)
                return result.Error
        }</span>

        // Update refresh times
        <span class="cov0" title="0">nsk.mu.Lock()
        nsk.lastRefresh = startTime
        if nsk.config.AutoRefresh &amp;&amp; nsk.refreshTicker != nil </span><span class="cov0" title="0">{
                interval, _ := time.ParseDuration(nsk.config.RefreshInterval)
                nsk.nextRefresh = time.Now().Add(interval)
        }</span>
        <span class="cov0" title="0">nsk.mu.Unlock()

        result.Success = true
        nsk.addRefreshResult(result)

        nsk.logger.Info("Kubeconfig refresh successful",
                "clusters", len(result.Clusters),
                "duration", time.Since(startTime))

        return nil</span>
}

// GetClusterKubeConfig gets kubeconfig for a specific cluster
func (nsk *NSKIntegration) GetClusterKubeConfig(ctx context.Context, clusterName string, save bool) (string, error) <span class="cov0" title="0">{
        nsk.logger.Info("Getting kubeconfig for cluster", "cluster", clusterName, "save", save)

        args := []string{"cluster", "kubeconfig", "--name", clusterName}
        if !save </span><span class="cov0" title="0">{
                // Output to stdout instead of saving to file
                args = append(args, "--stdout")
        }</span>

        <span class="cov0" title="0">cmd := exec.CommandContext(ctx, nsk.getNSKPath(), args...)
        cmd.Dir = nsk.config.ConfigDir
        cmd.Env = os.Environ()

        output, err := cmd.CombinedOutput()
        if err != nil </span><span class="cov0" title="0">{
                return "", fmt.Errorf("NSK get kubeconfig failed for cluster %s: %w, output: %s",
                        clusterName, err, output)
        }</span>

        <span class="cov0" title="0">if save </span><span class="cov0" title="0">{
                nsk.logger.Info("NSK kubeconfig saved for cluster", "cluster", clusterName)
                // Trigger cluster manager to rescan directory to pick up new file
                if err := nsk.clusterManager.RefreshClusters(ctx); err != nil </span><span class="cov0" title="0">{
                        nsk.logger.Error(err, "Failed to rediscover clusters after kubeconfig save")
                }</span>
        }

        <span class="cov0" title="0">return string(output), nil</span>
}

// refreshLoop runs the auto-refresh loop
func (nsk *NSKIntegration) refreshLoop(ctx context.Context) <span class="cov0" title="0">{
        for </span><span class="cov0" title="0">{
                select </span>{
                case &lt;-ctx.Done():<span class="cov0" title="0">
                        return</span>
                case &lt;-nsk.stopChan:<span class="cov0" title="0">
                        return</span>
                case &lt;-nsk.refreshTicker.C:<span class="cov0" title="0">
                        if err := nsk.RefreshKubeConfigs(ctx); err != nil </span><span class="cov0" title="0">{
                                nsk.logger.Error(err, "Auto-refresh of kubeconfigs failed")
                        }</span>
                }
        }
}

// getNSKPath returns the path to the NSK binary
func (nsk *NSKIntegration) getNSKPath() string <span class="cov3" title="3">{
        if nsk.config.NSKPath != "" </span><span class="cov1" title="1">{
                return nsk.config.NSKPath
        }</span>
        <span class="cov2" title="2">return "nsk"</span> // default to PATH lookup
}

// parseNSKOutput parses the NSK command output to extract cluster names
func (nsk *NSKIntegration) parseNSKOutput(output string) []string <span class="cov3" title="3">{
        var clusters []string
        lines := strings.Split(output, "\n")

        for _, line := range lines </span><span class="cov6" title="15">{
                line = strings.TrimSpace(line)
                // Look for lines that indicate a cluster was processed
                // This is a simplified parser - adjust based on actual NSK output format
                if strings.Contains(line, "kubeconfig saved") || strings.Contains(line, "cluster:") </span><span class="cov4" title="6">{
                        // Extract cluster name from the line
                        parts := strings.Fields(line)
                        for i, part := range parts </span><span class="cov6" title="16">{
                                if part == "cluster:" &amp;&amp; i+1 &lt; len(parts) </span><span class="cov3" title="4">{
                                        clusters = append(clusters, parts[i+1])
                                }</span>
                        }
                }
        }

        <span class="cov3" title="3">return clusters</span>
}

// addRefreshResult adds a refresh result to history
func (nsk *NSKIntegration) addRefreshResult(result RefreshResult) <span class="cov10" title="109">{
        nsk.mu.Lock()
        defer nsk.mu.Unlock()

        nsk.refreshHistory = append(nsk.refreshHistory, result)

        // Keep only last 100 results
        if len(nsk.refreshHistory) &gt; 100 </span><span class="cov4" title="5">{
                nsk.refreshHistory = nsk.refreshHistory[len(nsk.refreshHistory)-100:]
        }</span>
}

// GetLastRefreshTime returns the last refresh time
func (nsk *NSKIntegration) GetLastRefreshTime() time.Time <span class="cov2" title="2">{
        nsk.mu.RLock()
        defer nsk.mu.RUnlock()
        return nsk.lastRefresh
}</span>

// GetNextRefreshTime returns the next scheduled refresh time
func (nsk *NSKIntegration) GetNextRefreshTime() time.Time <span class="cov2" title="2">{
        nsk.mu.RLock()
        defer nsk.mu.RUnlock()
        return nsk.nextRefresh
}</span>

// GetRefreshHistory returns the refresh history
func (nsk *NSKIntegration) GetRefreshHistory() []RefreshResult <span class="cov2" title="2">{
        nsk.mu.RLock()
        defer nsk.mu.RUnlock()

        // Return a copy to avoid race conditions
        history := make([]RefreshResult, len(nsk.refreshHistory))
        copy(history, nsk.refreshHistory)
        return history
}</span>

// GetStatus returns the current NSK integration status
func (nsk *NSKIntegration) GetStatus() map[string]interface{} <span class="cov1" title="1">{
        nsk.mu.RLock()
        defer nsk.mu.RUnlock()

        status := map[string]interface{}{
                "enabled":      nsk.config.Enabled,
                "profile":      nsk.config.Profile,
                "rancher_url":  nsk.config.RancherURL,
                "config_dir":   nsk.config.ConfigDir,
                "auto_refresh": nsk.config.AutoRefresh,
                "last_refresh": nsk.lastRefresh,
                "next_refresh": nsk.nextRefresh,
        }

        // Add last refresh result
        if len(nsk.refreshHistory) &gt; 0 </span><span class="cov1" title="1">{
                lastResult := nsk.refreshHistory[len(nsk.refreshHistory)-1]
                status["last_result"] = map[string]interface{}{
                        "success":   lastResult.Success,
                        "timestamp": lastResult.Timestamp,
                        "clusters":  lastResult.Clusters,
                        "error":     "",
                }
                if lastResult.Error != nil </span><span class="cov0" title="0">{
                        status["last_result"].(map[string]interface{})["error"] = lastResult.Error.Error()
                }</span>
        }

        <span class="cov1" title="1">return status</span>
}

// ValidateNSKBinary validates that the NSK binary is available and executable
func (nsk *NSKIntegration) ValidateNSKBinary() error <span class="cov0" title="0">{
        nskPath := nsk.getNSKPath()

        // Check if binary exists
        fullPath, err := exec.LookPath(nskPath)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("NSK binary not found: %s", nskPath)
        }</span>

        // Check if executable
        <span class="cov0" title="0">if info, err := os.Stat(fullPath); err == nil </span><span class="cov0" title="0">{
                if info.Mode()&amp;0111 == 0 </span><span class="cov0" title="0">{
                        return fmt.Errorf("NSK binary not executable: %s", fullPath)
                }</span>
        }

        // Try to run version command to validate it works
        <span class="cov0" title="0">cmd := exec.Command(fullPath, "version")
        if output, err := cmd.CombinedOutput(); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("NSK binary validation failed: %w, output: %s", err, output)
        }</span>

        <span class="cov0" title="0">nsk.logger.Info("NSK binary validated", "path", fullPath)
        return nil</span>
}

// DiscoverClusters discovers new clusters from Rancher
func (nsk *NSKIntegration) DiscoverClusters(ctx context.Context, pattern string) ([]string, error) <span class="cov0" title="0">{
        args := []string{"cluster", "list"}
        if pattern != "" </span><span class="cov0" title="0">{
                args = append(args, "--name-pattern", pattern)
        }</span>

        <span class="cov0" title="0">cmd := exec.CommandContext(ctx, nsk.getNSKPath(), args...)
        cmd.Env = os.Environ()

        output, err := cmd.CombinedOutput()
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("NSK cluster discovery failed: %w, output: %s", err, output)
        }</span>

        // Parse output to get cluster names
        <span class="cov0" title="0">clusters := nsk.parseClusterList(string(output))
        return clusters, nil</span>
}

// parseClusterList parses the NSK cluster list output
func (nsk *NSKIntegration) parseClusterList(output string) []string <span class="cov3" title="3">{
        var clusters []string
        lines := strings.Split(output, "\n")

        for _, line := range lines </span><span class="cov5" title="11">{
                line = strings.TrimSpace(line)
                if line == "" || strings.HasPrefix(line, "#") </span><span class="cov4" title="5">{
                        continue</span>
                }

                // Simple parsing - adjust based on actual NSK output format
                // Assuming one cluster name per line
                <span class="cov4" title="6">clusters = append(clusters, line)</span>
        }

        <span class="cov3" title="3">return clusters</span>
}
</pre>
		
		<pre class="file" id="file19" style="display: none">package kubernetes

import (
        "context"

        "k8s.io/apimachinery/pkg/runtime/schema"
)

func (m *Manager) IsOpenShift(_ context.Context) bool <span class="cov0" title="0">{
        // This method should be fast and not block (it's called at startup)
        _, err := m.discoveryClient.ServerResourcesForGroupVersion(schema.GroupVersion{
                Group:   "project.openshift.io",
                Version: "v1",
        }.String())
        return err == nil
}</span>
</pre>
		
		<pre class="file" id="file20" style="display: none">package kubernetes

import (
        "bytes"
        "context"
        "errors"
        "fmt"

        v1 "k8s.io/api/core/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
        labelutil "k8s.io/apimachinery/pkg/labels"
        "k8s.io/apimachinery/pkg/runtime"
        "k8s.io/apimachinery/pkg/runtime/schema"
        "k8s.io/apimachinery/pkg/util/intstr"
        "k8s.io/apimachinery/pkg/util/rand"
        "k8s.io/client-go/tools/remotecommand"
        "k8s.io/metrics/pkg/apis/metrics"
        metricsv1beta1api "k8s.io/metrics/pkg/apis/metrics/v1beta1"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/version"
)

type PodsTopOptions struct {
        metav1.ListOptions
        AllNamespaces bool
        Namespace     string
        Name          string
}

func (k *Kubernetes) PodsListInAllNamespaces(ctx context.Context, options ResourceListOptions) (runtime.Unstructured, error) <span class="cov0" title="0">{
        return k.ResourcesList(ctx, &amp;schema.GroupVersionKind{
                Group: "", Version: "v1", Kind: "Pod",
        }, "", options)
}</span>

func (k *Kubernetes) PodsListInNamespace(ctx context.Context, namespace string, options ResourceListOptions) (runtime.Unstructured, error) <span class="cov0" title="0">{
        return k.ResourcesList(ctx, &amp;schema.GroupVersionKind{
                Group: "", Version: "v1", Kind: "Pod",
        }, namespace, options)
}</span>

func (k *Kubernetes) PodsGet(ctx context.Context, namespace, name string) (*unstructured.Unstructured, error) <span class="cov0" title="0">{
        return k.ResourcesGet(ctx, &amp;schema.GroupVersionKind{
                Group: "", Version: "v1", Kind: "Pod",
        }, k.NamespaceOrDefault(namespace), name)
}</span>

func (k *Kubernetes) PodsDelete(ctx context.Context, namespace, name string) (string, error) <span class="cov0" title="0">{
        namespace = k.NamespaceOrDefault(namespace)
        pod, err := k.ResourcesGet(ctx, &amp;schema.GroupVersionKind{Group: "", Version: "v1", Kind: "Pod"}, namespace, name)
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>

        <span class="cov0" title="0">isManaged := pod.GetLabels()[AppKubernetesManagedBy] == version.BinaryName
        managedLabelSelector := labelutil.Set{
                AppKubernetesManagedBy: version.BinaryName,
                AppKubernetesName:      pod.GetLabels()[AppKubernetesName],
        }.AsSelector()

        // Delete managed service
        if isManaged </span><span class="cov0" title="0">{
                services, err := k.manager.accessControlClientSet.Services(namespace)
                if err != nil </span><span class="cov0" title="0">{
                        return "", err
                }</span>
                <span class="cov0" title="0">if sl, _ := services.List(ctx, metav1.ListOptions{
                        LabelSelector: managedLabelSelector.String(),
                }); sl != nil </span><span class="cov0" title="0">{
                        for _, svc := range sl.Items </span><span class="cov0" title="0">{
                                _ = services.Delete(ctx, svc.Name, metav1.DeleteOptions{})
                        }</span>
                }
        }

        // Delete managed Route
        <span class="cov0" title="0">if isManaged &amp;&amp; k.supportsGroupVersion("route.openshift.io/v1") </span><span class="cov0" title="0">{
                routeResources := k.manager.dynamicClient.
                        Resource(schema.GroupVersionResource{Group: "route.openshift.io", Version: "v1", Resource: "routes"}).
                        Namespace(namespace)
                if rl, _ := routeResources.List(ctx, metav1.ListOptions{
                        LabelSelector: managedLabelSelector.String(),
                }); rl != nil </span><span class="cov0" title="0">{
                        for _, route := range rl.Items </span><span class="cov0" title="0">{
                                _ = routeResources.Delete(ctx, route.GetName(), metav1.DeleteOptions{})
                        }</span>
                }

        }
        <span class="cov0" title="0">return "Pod deleted successfully",
                k.ResourcesDelete(ctx, &amp;schema.GroupVersionKind{Group: "", Version: "v1", Kind: "Pod"}, namespace, name)</span>
}

func (k *Kubernetes) PodsLog(ctx context.Context, namespace, name, container string) (string, error) <span class="cov0" title="0">{
        tailLines := int64(256)
        pods, err := k.manager.accessControlClientSet.Pods(k.NamespaceOrDefault(namespace))
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">req := pods.GetLogs(name, &amp;v1.PodLogOptions{
                TailLines: &amp;tailLines,
                Container: container,
        })
        res := req.Do(ctx)
        if res.Error() != nil </span><span class="cov0" title="0">{
                return "", res.Error()
        }</span>
        <span class="cov0" title="0">rawData, err := res.Raw()
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">return string(rawData), nil</span>
}

func (k *Kubernetes) PodsRun(ctx context.Context, namespace, name, image string, port int32) ([]*unstructured.Unstructured, error) <span class="cov0" title="0">{
        if name == "" </span><span class="cov0" title="0">{
                name = version.BinaryName + "-run-" + rand.String(5)
        }</span>
        <span class="cov0" title="0">labels := map[string]string{
                AppKubernetesName:      name,
                AppKubernetesComponent: name,
                AppKubernetesManagedBy: version.BinaryName,
                AppKubernetesPartOf:    version.BinaryName + "-run-sandbox",
        }
        // NewPod
        var resources []any
        pod := &amp;v1.Pod{
                TypeMeta:   metav1.TypeMeta{APIVersion: "v1", Kind: "Pod"},
                ObjectMeta: metav1.ObjectMeta{Name: name, Namespace: k.NamespaceOrDefault(namespace), Labels: labels},
                Spec: v1.PodSpec{Containers: []v1.Container{{
                        Name:            name,
                        Image:           image,
                        ImagePullPolicy: v1.PullAlways,
                }}},
        }
        resources = append(resources, pod)
        if port &gt; 0 </span><span class="cov0" title="0">{
                pod.Spec.Containers[0].Ports = []v1.ContainerPort{{ContainerPort: port}}
                resources = append(resources, &amp;v1.Service{
                        TypeMeta:   metav1.TypeMeta{APIVersion: "v1", Kind: "Service"},
                        ObjectMeta: metav1.ObjectMeta{Name: name, Namespace: k.NamespaceOrDefault(namespace), Labels: labels},
                        Spec: v1.ServiceSpec{
                                Selector: labels,
                                Type:     v1.ServiceTypeClusterIP,
                                Ports:    []v1.ServicePort{{Port: port, TargetPort: intstr.FromInt32(port)}},
                        },
                })
        }</span>
        <span class="cov0" title="0">if port &gt; 0 &amp;&amp; k.supportsGroupVersion("route.openshift.io/v1") </span><span class="cov0" title="0">{
                resources = append(resources, &amp;unstructured.Unstructured{
                        Object: map[string]interface{}{
                                "apiVersion": "route.openshift.io/v1",
                                "kind":       "Route",
                                "metadata": map[string]interface{}{
                                        "name":      name,
                                        "namespace": k.NamespaceOrDefault(namespace),
                                        "labels":    labels,
                                },
                                "spec": map[string]interface{}{
                                        "to": map[string]interface{}{
                                                "kind":   "Service",
                                                "name":   name,
                                                "weight": 100,
                                        },
                                        "port": map[string]interface{}{
                                                "targetPort": intstr.FromInt32(port),
                                        },
                                        "tls": map[string]interface{}{
                                                "termination":                   "edge",
                                                "insecureEdgeTerminationPolicy": "Redirect",
                                        },
                                },
                        },
                })

        }</span>

        // Convert the objects to Unstructured and reuse resourcesCreateOrUpdate functionality
        <span class="cov0" title="0">converter := runtime.DefaultUnstructuredConverter
        var toCreate []*unstructured.Unstructured
        for _, obj := range resources </span><span class="cov0" title="0">{
                m, err := converter.ToUnstructured(obj)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>
                <span class="cov0" title="0">u := &amp;unstructured.Unstructured{}
                if err = converter.FromUnstructured(m, u); err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>
                <span class="cov0" title="0">toCreate = append(toCreate, u)</span>
        }
        <span class="cov0" title="0">return k.resourcesCreateOrUpdate(ctx, toCreate)</span>
}

func (k *Kubernetes) PodsTop(ctx context.Context, options PodsTopOptions) (*metrics.PodMetricsList, error) <span class="cov0" title="0">{
        // TODO, maybe move to mcp Tools setup and omit in case metrics aren't available in the target cluster
        if !k.supportsGroupVersion(metrics.GroupName + "/" + metricsv1beta1api.SchemeGroupVersion.Version) </span><span class="cov0" title="0">{
                return nil, errors.New("metrics API is not available")
        }</span>
        <span class="cov0" title="0">namespace := options.Namespace
        if options.AllNamespaces &amp;&amp; namespace == "" </span><span class="cov0" title="0">{
                namespace = ""
        }</span> else<span class="cov0" title="0"> {
                namespace = k.NamespaceOrDefault(namespace)
        }</span>
        <span class="cov0" title="0">return k.manager.accessControlClientSet.PodsMetricses(ctx, namespace, options.Name, options.ListOptions)</span>
}

func (k *Kubernetes) PodsExec(ctx context.Context, namespace, name, container string, command []string) (string, error) <span class="cov0" title="0">{
        namespace = k.NamespaceOrDefault(namespace)
        pods, err := k.manager.accessControlClientSet.Pods(namespace)
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">pod, err := pods.Get(ctx, name, metav1.GetOptions{})
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        // https://github.com/kubernetes/kubectl/blob/5366de04e168bcbc11f5e340d131a9ca8b7d0df4/pkg/cmd/exec/exec.go#L350-L352
        <span class="cov0" title="0">if pod.Status.Phase == v1.PodSucceeded || pod.Status.Phase == v1.PodFailed </span><span class="cov0" title="0">{
                return "", fmt.Errorf("cannot exec into a container in a completed pod; current phase is %s", pod.Status.Phase)
        }</span>
        <span class="cov0" title="0">if container == "" </span><span class="cov0" title="0">{
                container = pod.Spec.Containers[0].Name
        }</span>
        <span class="cov0" title="0">podExecOptions := &amp;v1.PodExecOptions{
                Container: container,
                Command:   command,
                Stdout:    true,
                Stderr:    true,
        }
        executor, err := k.manager.accessControlClientSet.PodsExec(namespace, name, podExecOptions)
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">stdout := bytes.NewBuffer(make([]byte, 0))
        stderr := bytes.NewBuffer(make([]byte, 0))
        if err = executor.StreamWithContext(ctx, remotecommand.StreamOptions{
                Stdout: stdout, Stderr: stderr, Tty: false,
        }); err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">if stdout.Len() &gt; 0 </span><span class="cov0" title="0">{
                return stdout.String(), nil
        }</span>
        <span class="cov0" title="0">if stderr.Len() &gt; 0 </span><span class="cov0" title="0">{
                return stderr.String(), nil
        }</span>
        <span class="cov0" title="0">return "", nil</span>
}
</pre>
		
		<pre class="file" id="file21" style="display: none">package kubernetes

import (
        "context"
        "fmt"
        "k8s.io/apimachinery/pkg/runtime"
        "regexp"
        "strings"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/version"
        authv1 "k8s.io/api/authorization/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
        metav1beta1 "k8s.io/apimachinery/pkg/apis/meta/v1beta1"
        "k8s.io/apimachinery/pkg/runtime/schema"
        "k8s.io/apimachinery/pkg/util/yaml"
)

const (
        AppKubernetesComponent = "app.kubernetes.io/component"
        AppKubernetesManagedBy = "app.kubernetes.io/managed-by"
        AppKubernetesName      = "app.kubernetes.io/name"
        AppKubernetesPartOf    = "app.kubernetes.io/part-of"
)

type ResourceListOptions struct {
        metav1.ListOptions
        AsTable bool
}

func (k *Kubernetes) ResourcesList(ctx context.Context, gvk *schema.GroupVersionKind, namespace string, options ResourceListOptions) (runtime.Unstructured, error) <span class="cov0" title="0">{
        gvr, err := k.resourceFor(gvk)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // Check if operation is allowed for all namespaces (applicable for namespaced resources)
        <span class="cov0" title="0">isNamespaced, _ := k.isNamespaced(gvk)
        if isNamespaced &amp;&amp; !k.canIUse(ctx, gvr, namespace, "list") &amp;&amp; namespace == "" </span><span class="cov0" title="0">{
                namespace = k.manager.configuredNamespace()
        }</span>
        <span class="cov0" title="0">if options.AsTable </span><span class="cov0" title="0">{
                return k.resourcesListAsTable(ctx, gvk, gvr, namespace, options)
        }</span>
        <span class="cov0" title="0">return k.manager.dynamicClient.Resource(*gvr).Namespace(namespace).List(ctx, options.ListOptions)</span>
}

func (k *Kubernetes) ResourcesGet(ctx context.Context, gvk *schema.GroupVersionKind, namespace, name string) (*unstructured.Unstructured, error) <span class="cov0" title="0">{
        gvr, err := k.resourceFor(gvk)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // If it's a namespaced resource and namespace wasn't provided, try to use the default configured one
        <span class="cov0" title="0">if namespaced, nsErr := k.isNamespaced(gvk); nsErr == nil &amp;&amp; namespaced </span><span class="cov0" title="0">{
                namespace = k.NamespaceOrDefault(namespace)
        }</span>
        <span class="cov0" title="0">return k.manager.dynamicClient.Resource(*gvr).Namespace(namespace).Get(ctx, name, metav1.GetOptions{})</span>
}

func (k *Kubernetes) ResourcesCreateOrUpdate(ctx context.Context, resource string) ([]*unstructured.Unstructured, error) <span class="cov0" title="0">{
        separator := regexp.MustCompile(`\r?\n---\r?\n`)
        resources := separator.Split(resource, -1)
        var parsedResources []*unstructured.Unstructured
        for _, r := range resources </span><span class="cov0" title="0">{
                var obj unstructured.Unstructured
                if err := yaml.NewYAMLToJSONDecoder(strings.NewReader(r)).Decode(&amp;obj); err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>
                <span class="cov0" title="0">parsedResources = append(parsedResources, &amp;obj)</span>
        }
        <span class="cov0" title="0">return k.resourcesCreateOrUpdate(ctx, parsedResources)</span>
}

func (k *Kubernetes) ResourcesDelete(ctx context.Context, gvk *schema.GroupVersionKind, namespace, name string) error <span class="cov0" title="0">{
        gvr, err := k.resourceFor(gvk)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // If it's a namespaced resource and namespace wasn't provided, try to use the default configured one
        <span class="cov0" title="0">if namespaced, nsErr := k.isNamespaced(gvk); nsErr == nil &amp;&amp; namespaced </span><span class="cov0" title="0">{
                namespace = k.NamespaceOrDefault(namespace)
        }</span>
        <span class="cov0" title="0">return k.manager.dynamicClient.Resource(*gvr).Namespace(namespace).Delete(ctx, name, metav1.DeleteOptions{})</span>
}

// resourcesListAsTable retrieves a list of resources in a table format.
// It's almost identical to the dynamic.DynamicClient implementation, but it uses a specific Accept header to request the table format.
// dynamic.DynamicClient does not provide a way to set the HTTP header (TODO: create an issue to request this feature)
func (k *Kubernetes) resourcesListAsTable(ctx context.Context, gvk *schema.GroupVersionKind, gvr *schema.GroupVersionResource, namespace string, options ResourceListOptions) (runtime.Unstructured, error) <span class="cov0" title="0">{
        var url []string
        if len(gvr.Group) == 0 </span><span class="cov0" title="0">{
                url = append(url, "api")
        }</span> else<span class="cov0" title="0"> {
                url = append(url, "apis", gvr.Group)
        }</span>
        <span class="cov0" title="0">url = append(url, gvr.Version)
        if len(namespace) &gt; 0 </span><span class="cov0" title="0">{
                url = append(url, "namespaces", namespace)
        }</span>
        <span class="cov0" title="0">url = append(url, gvr.Resource)
        var table metav1.Table
        err := k.manager.discoveryClient.RESTClient().
                Get().
                SetHeader("Accept", strings.Join([]string{
                        fmt.Sprintf("application/json;as=Table;v=%s;g=%s", metav1.SchemeGroupVersion.Version, metav1.GroupName),
                        fmt.Sprintf("application/json;as=Table;v=%s;g=%s", metav1beta1.SchemeGroupVersion.Version, metav1beta1.GroupName),
                        "application/json",
                }, ",")).
                AbsPath(url...).
                SpecificallyVersionedParams(&amp;options.ListOptions, ParameterCodec, schema.GroupVersion{Version: "v1"}).
                Do(ctx).Into(&amp;table)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        // Add metav1.Table apiVersion and kind to the unstructured object (server may not return these fields)
        <span class="cov0" title="0">table.SetGroupVersionKind(metav1.SchemeGroupVersion.WithKind("Table"))
        // Add additional columns for fields that aren't returned by the server
        table.ColumnDefinitions = append([]metav1.TableColumnDefinition{
                {Name: "apiVersion", Type: "string"},
                {Name: "kind", Type: "string"},
        }, table.ColumnDefinitions...)
        for i := range table.Rows </span><span class="cov0" title="0">{
                row := &amp;table.Rows[i]
                row.Cells = append([]interface{}{
                        gvr.GroupVersion().String(),
                        gvk.Kind,
                }, row.Cells...)
        }</span>
        <span class="cov0" title="0">unstructuredObject, err := runtime.DefaultUnstructuredConverter.ToUnstructured(&amp;table)
        return &amp;unstructured.Unstructured{Object: unstructuredObject}, err</span>
}

func (k *Kubernetes) resourcesCreateOrUpdate(ctx context.Context, resources []*unstructured.Unstructured) ([]*unstructured.Unstructured, error) <span class="cov0" title="0">{
        for i, obj := range resources </span><span class="cov0" title="0">{
                gvk := obj.GroupVersionKind()
                gvr, rErr := k.resourceFor(&amp;gvk)
                if rErr != nil </span><span class="cov0" title="0">{
                        return nil, rErr
                }</span>

                <span class="cov0" title="0">namespace := obj.GetNamespace()
                // If it's a namespaced resource and namespace wasn't provided, try to use the default configured one
                if namespaced, nsErr := k.isNamespaced(&amp;gvk); nsErr == nil &amp;&amp; namespaced </span><span class="cov0" title="0">{
                        namespace = k.NamespaceOrDefault(namespace)
                }</span>
                <span class="cov0" title="0">resources[i], rErr = k.manager.dynamicClient.Resource(*gvr).Namespace(namespace).Apply(ctx, obj.GetName(), obj, metav1.ApplyOptions{
                        FieldManager: version.BinaryName,
                })
                if rErr != nil </span><span class="cov0" title="0">{
                        return nil, rErr
                }</span>
                // Clear the cache to ensure the next operation is performed on the latest exposed APIs (will change after the CRD creation)
                <span class="cov0" title="0">if gvk.Kind == "CustomResourceDefinition" </span><span class="cov0" title="0">{
                        k.manager.accessControlRESTMapper.Reset()
                }</span>
        }
        <span class="cov0" title="0">return resources, nil</span>
}

func (k *Kubernetes) resourceFor(gvk *schema.GroupVersionKind) (*schema.GroupVersionResource, error) <span class="cov0" title="0">{
        m, err := k.manager.accessControlRESTMapper.RESTMapping(schema.GroupKind{Group: gvk.Group, Kind: gvk.Kind}, gvk.Version)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov0" title="0">return &amp;m.Resource, nil</span>
}

func (k *Kubernetes) isNamespaced(gvk *schema.GroupVersionKind) (bool, error) <span class="cov0" title="0">{
        apiResourceList, err := k.manager.discoveryClient.ServerResourcesForGroupVersion(gvk.GroupVersion().String())
        if err != nil </span><span class="cov0" title="0">{
                return false, err
        }</span>
        <span class="cov0" title="0">for _, apiResource := range apiResourceList.APIResources </span><span class="cov0" title="0">{
                if apiResource.Kind == gvk.Kind </span><span class="cov0" title="0">{
                        return apiResource.Namespaced, nil
                }</span>
        }
        <span class="cov0" title="0">return false, nil</span>
}

func (k *Kubernetes) supportsGroupVersion(groupVersion string) bool <span class="cov0" title="0">{
        if _, err := k.manager.discoveryClient.ServerResourcesForGroupVersion(groupVersion); err != nil </span><span class="cov0" title="0">{
                return false
        }</span>
        <span class="cov0" title="0">return true</span>
}

func (k *Kubernetes) canIUse(ctx context.Context, gvr *schema.GroupVersionResource, namespace, verb string) bool <span class="cov0" title="0">{
        accessReviews, err := k.manager.accessControlClientSet.SelfSubjectAccessReviews()
        if err != nil </span><span class="cov0" title="0">{
                return false
        }</span>
        <span class="cov0" title="0">response, err := accessReviews.Create(ctx, &amp;authv1.SelfSubjectAccessReview{
                Spec: authv1.SelfSubjectAccessReviewSpec{ResourceAttributes: &amp;authv1.ResourceAttributes{
                        Namespace: namespace,
                        Verb:      verb,
                        Group:     gvr.Group,
                        Version:   gvr.Version,
                        Resource:  gvr.Resource,
                }},
        }, metav1.CreateOptions{})
        if err != nil </span><span class="cov0" title="0">{
                // TODO: maybe return the error too
                return false
        }</span>
        <span class="cov0" title="0">return response.Status.Allowed</span>
}
</pre>
		
		<pre class="file" id="file22" style="display: none">package kubernetes

import (
        "context"
        "fmt"

        authenticationv1api "k8s.io/api/authentication/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func (m *Manager) VerifyToken(ctx context.Context, token, audience string) (*authenticationv1api.UserInfo, []string, error) <span class="cov0" title="0">{
        tokenReviewClient, err := m.accessControlClientSet.TokenReview()
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov0" title="0">tokenReview := &amp;authenticationv1api.TokenReview{
                TypeMeta: metav1.TypeMeta{
                        APIVersion: "authentication.k8s.io/v1",
                        Kind:       "TokenReview",
                },
                Spec: authenticationv1api.TokenReviewSpec{
                        Token:     token,
                        Audiences: []string{audience},
                },
        }

        result, err := tokenReviewClient.Create(ctx, tokenReview, metav1.CreateOptions{})
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, fmt.Errorf("failed to create token review: %v", err)
        }</span>

        <span class="cov0" title="0">if !result.Status.Authenticated </span><span class="cov0" title="0">{
                if result.Status.Error != "" </span><span class="cov0" title="0">{
                        return nil, nil, fmt.Errorf("token authentication failed: %s", result.Status.Error)
                }</span>
                <span class="cov0" title="0">return nil, nil, fmt.Errorf("token authentication failed")</span>
        }

        <span class="cov0" title="0">return &amp;result.Status.User, result.Status.Audiences, nil</span>
}
</pre>
		
		<pre class="file" id="file23" style="display: none">package nsk

import (
        "context"
        "encoding/json"
        "fmt"
        "os"
        "os/exec"
        "path/filepath"
        "regexp"
        "strings"
        "sync"
        "time"

        "k8s.io/klog/v2"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config"
)

// NSKClient provides integration with the NSK (Netskope Kubernetes) tool
type NSKClient struct {
        config      *config.NSKConfig
        nskPath     string
        logger      klog.Logger
        lastRefresh time.Time
        clusters    map[string]*ClusterInfo
        mutex       sync.RWMutex

        // Resource cleanup
        ctx          context.Context
        cancel       context.CancelFunc
        cleanupFuncs []func() error
}

// ClusterInfo represents information about a discovered cluster
type ClusterInfo struct {
        Name        string    `json:"name"`
        KubeConfig  string    `json:"kubeconfig_path"`
        LastRefresh time.Time `json:"last_refresh"`
        Status      string    `json:"status"`
        Environment string    `json:"environment,omitempty"`
        Description string    `json:"description,omitempty"`
}

// NSKClusterListResponse represents the JSON response from `nsk cluster list --output json`
type NSKClusterListResponse struct {
        Clusters []NSKCluster `json:"clusters"`
}

// NSKCluster represents a cluster entry from NSK
type NSKCluster struct {
        Name        string `json:"name"`
        ID          string `json:"id"`
        State       string `json:"state"`
        Description string `json:"description"`
}

// sanitizeClusterName sanitizes cluster names to prevent command injection
// Only allows alphanumeric characters, dashes, underscores, and dots
func sanitizeClusterName(name string) string <span class="cov10" title="27">{
        if name == "" </span><span class="cov1" title="1">{
                return ""
        }</span>

        // Allow only safe characters: alphanumeric, dash, underscore, dot
        <span class="cov9" title="26">reg := regexp.MustCompile("[^a-zA-Z0-9_.-]")
        sanitized := reg.ReplaceAllString(name, "")

        // Ensure the name is not empty after sanitization
        if sanitized == "" </span><span class="cov1" title="1">{
                return "invalid-cluster-name"
        }</span>

        // Limit length to prevent excessive resource usage
        <span class="cov9" title="25">if len(sanitized) &gt; 128 </span><span class="cov0" title="0">{
                sanitized = sanitized[:128]
        }</span>

        <span class="cov9" title="25">return sanitized</span>
}

// NewNSKClient creates a new NSK client with the given configuration
func NewNSKClient(config *config.NSKConfig, logger klog.Logger) (*NSKClient, error) <span class="cov0" title="0">{
        if config == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("NSK configuration is required")
        }</span>

        <span class="cov0" title="0">nskPath := config.NSKPath
        if nskPath == "" </span><span class="cov0" title="0">{
                nskPath = "nsk"
        }</span>

        // Verify NSK binary is available
        <span class="cov0" title="0">if _, err := exec.LookPath(nskPath); err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("NSK binary not found in PATH: %w", err)
        }</span>

        <span class="cov0" title="0">ctx, cancel := context.WithCancel(context.Background())
        client := &amp;NSKClient{
                config:       config,
                nskPath:      nskPath,
                logger:       logger,
                clusters:     make(map[string]*ClusterInfo),
                ctx:          ctx,
                cancel:       cancel,
                cleanupFuncs: make([]func() error, 0),
        }

        // Ensure config directory exists
        if config.ConfigDir != "" </span><span class="cov0" title="0">{
                if err := os.MkdirAll(config.ConfigDir, 0755); err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to create config directory %s: %w", config.ConfigDir, err)
                }</span>
        }

        <span class="cov0" title="0">return client, nil</span>
}

// DiscoverClusters discovers available clusters using NSK
func (c *NSKClient) DiscoverClusters(ctx context.Context) error <span class="cov0" title="0">{
        c.logger.V(2).Info("Discovering clusters via NSK")

        // Use the more restrictive context
        useCtx := ctx
        if c.ctx.Err() != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("NSK client is closed")
        }</span>
        <span class="cov0" title="0">if ctx.Err() != nil </span><span class="cov0" title="0">{
                useCtx = c.ctx
        }</span>

        // Execute nsk cluster list --output json
        <span class="cov0" title="0">cmd := exec.CommandContext(useCtx, c.nskPath, "cluster", "list", "--output", "json")

        // Set environment variables
        env := c.buildEnvironment()
        cmd.Env = env

        output, err := cmd.Output()
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to list clusters: %w", err)
        }</span>

        // Parse JSON response
        <span class="cov0" title="0">var response NSKClusterListResponse
        if err := json.Unmarshal(output, &amp;response); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to parse cluster list response: %w", err)
        }</span>

        // Filter and process clusters
        <span class="cov0" title="0">discoveredClusters := make(map[string]*ClusterInfo)
        for _, cluster := range response.Clusters </span><span class="cov0" title="0">{
                if c.shouldIncludeCluster(cluster.Name) </span><span class="cov0" title="0">{
                        clusterInfo := &amp;ClusterInfo{
                                Name:        cluster.Name,
                                Status:      cluster.State,
                                Environment: c.detectEnvironment(cluster.Name),
                                Description: cluster.Description,
                                LastRefresh: time.Now(),
                        }

                        // Set kubeconfig path
                        if c.config.ConfigDir != "" </span><span class="cov0" title="0">{
                                clusterInfo.KubeConfig = filepath.Join(c.config.ConfigDir, fmt.Sprintf("%s.yaml", cluster.Name))
                        }</span>

                        <span class="cov0" title="0">discoveredClusters[cluster.Name] = clusterInfo</span>
                }
        }

        // Update clusters map
        <span class="cov0" title="0">c.mutex.Lock()
        c.clusters = discoveredClusters
        c.lastRefresh = time.Now()
        c.mutex.Unlock()

        c.logger.V(2).Info("Cluster discovery completed", "count", len(discoveredClusters))
        return nil</span>
}

// DownloadKubeConfigs downloads kubeconfig files for all discovered clusters
func (c *NSKClient) DownloadKubeConfigs(ctx context.Context) error <span class="cov0" title="0">{
        c.mutex.RLock()
        clusters := make(map[string]*ClusterInfo)
        for k, v := range c.clusters </span><span class="cov0" title="0">{
                clusters[k] = v
        }</span>
        <span class="cov0" title="0">c.mutex.RUnlock()

        if len(clusters) == 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("no clusters discovered, run DiscoverClusters first")
        }</span>

        <span class="cov0" title="0">c.logger.V(2).Info("Downloading kubeconfig files", "count", len(clusters))

        // Download kubeconfigs for each cluster
        for name, clusterInfo := range clusters </span><span class="cov0" title="0">{
                if err := c.downloadClusterKubeConfig(ctx, name, clusterInfo.KubeConfig); err != nil </span><span class="cov0" title="0">{
                        c.logger.Error(err, "Failed to download kubeconfig", "cluster", name)
                        // Continue with other clusters even if one fails
                        continue</span>
                }

                <span class="cov0" title="0">c.logger.V(3).Info("Downloaded kubeconfig", "cluster", name, "path", clusterInfo.KubeConfig)</span>
        }

        <span class="cov0" title="0">return nil</span>
}

// downloadClusterKubeConfig downloads kubeconfig for a specific cluster
func (c *NSKClient) downloadClusterKubeConfig(ctx context.Context, clusterName, outputPath string) error <span class="cov0" title="0">{
        // Sanitize cluster name to prevent command injection
        sanitizedClusterName := sanitizeClusterName(clusterName)
        if sanitizedClusterName != clusterName </span><span class="cov0" title="0">{
                c.logger.V(1).Info("Cluster name was sanitized", "original", clusterName, "sanitized", sanitizedClusterName)
        }</span>

        // Validate output path to prevent path traversal
        <span class="cov0" title="0">if !filepath.IsAbs(outputPath) || strings.Contains(outputPath, "..") </span><span class="cov0" title="0">{
                return fmt.Errorf("invalid output path: %s", outputPath)
        }</span>

        // Use the more restrictive context
        <span class="cov0" title="0">useCtx := ctx
        if c.ctx.Err() != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("NSK client is closed")
        }</span>
        <span class="cov0" title="0">if ctx.Err() != nil </span><span class="cov0" title="0">{
                useCtx = c.ctx
        }</span>

        // Execute nsk cluster kubeconfig --name=&lt;cluster&gt; --output=&lt;path&gt;
        <span class="cov0" title="0">cmd := exec.CommandContext(useCtx, c.nskPath, "cluster", "kubeconfig",
                fmt.Sprintf("--name=%s", sanitizedClusterName),
                fmt.Sprintf("--output=%s", outputPath))

        // Set environment variables
        env := c.buildEnvironment()
        cmd.Env = env

        if err := cmd.Run(); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to download kubeconfig for cluster %s: %w", clusterName, err)
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// GetClusters returns a copy of the discovered clusters
func (c *NSKClient) GetClusters() map[string]*ClusterInfo <span class="cov0" title="0">{
        c.mutex.RLock()
        defer c.mutex.RUnlock()

        clusters := make(map[string]*ClusterInfo)
        for k, v := range c.clusters </span><span class="cov0" title="0">{
                // Create a copy
                clusters[k] = &amp;ClusterInfo{
                        Name:        v.Name,
                        KubeConfig:  v.KubeConfig,
                        LastRefresh: v.LastRefresh,
                        Status:      v.Status,
                        Environment: v.Environment,
                        Description: v.Description,
                }
        }</span>
        <span class="cov0" title="0">return clusters</span>
}

// GetCluster returns information about a specific cluster
func (c *NSKClient) GetCluster(name string) (*ClusterInfo, error) <span class="cov0" title="0">{
        c.mutex.RLock()
        defer c.mutex.RUnlock()

        cluster, exists := c.clusters[name]
        if !exists </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("cluster %s not found", name)
        }</span>

        // Return a copy
        <span class="cov0" title="0">return &amp;ClusterInfo{
                Name:        cluster.Name,
                KubeConfig:  cluster.KubeConfig,
                LastRefresh: cluster.LastRefresh,
                Status:      cluster.Status,
                Environment: cluster.Environment,
                Description: cluster.Description,
        }, nil</span>
}

// RefreshIfNeeded refreshes cluster information if auto-refresh is enabled and interval has passed
func (c *NSKClient) RefreshIfNeeded(ctx context.Context) error <span class="cov0" title="0">{
        if !c.config.AutoRefresh </span><span class="cov0" title="0">{
                return nil
        }</span>

        <span class="cov0" title="0">refreshInterval, err := time.ParseDuration(c.config.RefreshInterval)
        if err != nil </span><span class="cov0" title="0">{
                c.logger.Error(err, "Invalid refresh interval", "interval", c.config.RefreshInterval)
                return err
        }</span>

        <span class="cov0" title="0">c.mutex.RLock()
        needsRefresh := time.Since(c.lastRefresh) &gt; refreshInterval
        c.mutex.RUnlock()

        if needsRefresh </span><span class="cov0" title="0">{
                c.logger.V(2).Info("Auto-refreshing cluster information")
                if err := c.DiscoverClusters(ctx); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov0" title="0">return c.DownloadKubeConfigs(ctx)</span>
        }

        <span class="cov0" title="0">return nil</span>
}

// shouldIncludeCluster determines if a cluster should be included based on configuration filters
func (c *NSKClient) shouldIncludeCluster(clusterName string) bool <span class="cov0" title="0">{
        // Check exclusion list first
        for _, excluded := range c.config.ExcludeClusters </span><span class="cov0" title="0">{
                if clusterName == excluded </span><span class="cov0" title="0">{
                        return false
                }</span>
        }

        // Check inclusion list (if specified)
        <span class="cov0" title="0">if len(c.config.IncludeClusters) &gt; 0 </span><span class="cov0" title="0">{
                found := false
                for _, included := range c.config.IncludeClusters </span><span class="cov0" title="0">{
                        if clusterName == included </span><span class="cov0" title="0">{
                                found = true
                                break</span>
                        }
                }
                <span class="cov0" title="0">if !found </span><span class="cov0" title="0">{
                        return false
                }</span>
        }

        // Check cluster pattern (if specified)
        <span class="cov0" title="0">if c.config.ClusterPattern != "" </span><span class="cov0" title="0">{
                matched, err := regexp.MatchString(c.config.ClusterPattern, clusterName)
                if err != nil </span><span class="cov0" title="0">{
                        c.logger.Error(err, "Invalid cluster pattern", "pattern", c.config.ClusterPattern)
                        return false
                }</span>
                <span class="cov0" title="0">return matched</span>
        }

        <span class="cov0" title="0">return true</span>
}

// detectEnvironment attempts to detect the environment based on cluster name
func (c *NSKClient) detectEnvironment(clusterName string) string <span class="cov0" title="0">{
        name := strings.ToLower(clusterName)

        if strings.Contains(name, "prod") || strings.Contains(name, "production") </span><span class="cov0" title="0">{
                return "production"
        }</span>
        <span class="cov0" title="0">if strings.Contains(name, "stag") || strings.Contains(name, "staging") </span><span class="cov0" title="0">{
                return "staging"
        }</span>
        <span class="cov0" title="0">if strings.Contains(name, "dev") || strings.Contains(name, "development") </span><span class="cov0" title="0">{
                return "development"
        }</span>
        <span class="cov0" title="0">if strings.Contains(name, "test") || strings.Contains(name, "testing") </span><span class="cov0" title="0">{
                return "testing"
        }</span>

        <span class="cov0" title="0">return "unknown"</span>
}

// buildEnvironment builds the environment variables for NSK command execution
func (c *NSKClient) buildEnvironment() []string <span class="cov0" title="0">{
        env := os.Environ()

        // Add NSK-specific environment variables
        if c.config.RancherURL != "" </span><span class="cov0" title="0">{
                env = append(env, fmt.Sprintf("RANCHER_URL=%s", c.config.RancherURL))
        }</span>
        <span class="cov0" title="0">if c.config.RancherToken != "" </span><span class="cov0" title="0">{
                env = append(env, fmt.Sprintf("RANCHER_TOKEN=%s", c.config.RancherToken))
        }</span>
        <span class="cov0" title="0">if c.config.Profile != "" </span><span class="cov0" title="0">{
                env = append(env, fmt.Sprintf("NSK_PROFILE=%s", c.config.Profile))
        }</span>
        <span class="cov0" title="0">if c.config.ConfigDir != "" </span><span class="cov0" title="0">{
                env = append(env, fmt.Sprintf("NSK_CONFIG_DIR=%s", c.config.ConfigDir))
        }</span>

        // Add custom environment variables from configuration
        <span class="cov0" title="0">for key, value := range c.config.Environment </span><span class="cov0" title="0">{
                env = append(env, fmt.Sprintf("%s=%s", key, value))
        }</span>

        <span class="cov0" title="0">return env</span>
}

// Close properly closes the NSK client and cleans up resources
func (c *NSKClient) Close() error <span class="cov0" title="0">{
        // Cancel context to stop any running operations
        if c.cancel != nil </span><span class="cov0" title="0">{
                c.cancel()
        }</span>

        // Run all cleanup functions
        <span class="cov0" title="0">var errors []error
        for _, cleanup := range c.cleanupFuncs </span><span class="cov0" title="0">{
                if err := cleanup(); err != nil </span><span class="cov0" title="0">{
                        errors = append(errors, err)
                }</span>
        }

        // Clear clusters map
        <span class="cov0" title="0">c.mutex.Lock()
        c.clusters = make(map[string]*ClusterInfo)
        c.mutex.Unlock()

        if len(errors) &gt; 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("cleanup errors: %v", errors)
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// RegisterCleanup registers a cleanup function to be called when the client is closed
func (c *NSKClient) RegisterCleanup(cleanup func() error) <span class="cov0" title="0">{
        c.cleanupFuncs = append(c.cleanupFuncs, cleanup)
}</span>

// GetContext returns the client's context for long-running operations
func (c *NSKClient) GetContext() context.Context <span class="cov0" title="0">{
        return c.ctx
}</span>

// IsHealthy performs a basic health check of the NSK integration
func (c *NSKClient) IsHealthy(ctx context.Context) error <span class="cov0" title="0">{
        // Check if NSK binary is available
        if _, err := exec.LookPath(c.nskPath); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("NSK binary not available: %w", err)
        }</span>

        // Try to execute a simple NSK command
        <span class="cov0" title="0">cmd := exec.CommandContext(ctx, c.nskPath, "version")
        env := c.buildEnvironment()
        cmd.Env = env

        if err := cmd.Run(); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("NSK command execution failed: %w", err)
        }</span>

        <span class="cov0" title="0">return nil</span>
}
</pre>
		
		<pre class="file" id="file24" style="display: none">package nsk

import (
        "context"
        "fmt"
        "os"
        "sync"
        "time"

        "k8s.io/klog/v2"

        "github.com/netSkopePlatformEng/kubernetes-mcp-server/pkg/config"
)

// Manager provides high-level NSK integration functionality
type Manager struct {
        client        *NSKClient
        config        *config.NSKConfig
        logger        klog.Logger
        refreshTicker *time.Ticker
        stopChan      chan struct{}
        mutex         sync.RWMutex
        running       bool
}

// NewManager creates a new NSK manager
func NewManager(config *config.NSKConfig, logger klog.Logger) (*Manager, error) <span class="cov0" title="0">{
        client, err := NewNSKClient(config, logger)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to create NSK client: %w", err)
        }</span>

        <span class="cov0" title="0">return &amp;Manager{
                client:   client,
                config:   config,
                logger:   logger,
                stopChan: make(chan struct{}),
        }, nil</span>
}

// Start begins the NSK manager operations
func (m *Manager) Start(ctx context.Context) error <span class="cov0" title="0">{
        m.mutex.Lock()
        defer m.mutex.Unlock()

        if m.running </span><span class="cov0" title="0">{
                return fmt.Errorf("NSK manager is already running")
        }</span>

        <span class="cov0" title="0">m.logger.Info("Starting NSK manager")

        // Initial cluster discovery and kubeconfig download
        if err := m.client.DiscoverClusters(ctx); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("initial cluster discovery failed: %w", err)
        }</span>

        <span class="cov0" title="0">if err := m.client.DownloadKubeConfigs(ctx); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("initial kubeconfig download failed: %w", err)
        }</span>

        // Start auto-refresh if enabled
        <span class="cov0" title="0">if m.config.AutoRefresh </span><span class="cov0" title="0">{
                refreshInterval, err := time.ParseDuration(m.config.RefreshInterval)
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("invalid refresh interval: %w", err)
                }</span>

                <span class="cov0" title="0">m.refreshTicker = time.NewTicker(refreshInterval)
                go m.refreshLoop(ctx)</span>
        }

        <span class="cov0" title="0">m.running = true
        m.logger.Info("NSK manager started successfully")
        return nil</span>
}

// Stop stops the NSK manager operations
func (m *Manager) Stop() <span class="cov0" title="0">{
        m.mutex.Lock()
        defer m.mutex.Unlock()

        if !m.running </span><span class="cov0" title="0">{
                return
        }</span>

        <span class="cov0" title="0">m.logger.Info("Stopping NSK manager")

        close(m.stopChan)
        if m.refreshTicker != nil </span><span class="cov0" title="0">{
                m.refreshTicker.Stop()
        }</span>

        <span class="cov0" title="0">m.running = false
        m.logger.Info("NSK manager stopped")</span>
}

// refreshLoop handles periodic refresh of cluster information
func (m *Manager) refreshLoop(ctx context.Context) <span class="cov0" title="0">{
        for </span><span class="cov0" title="0">{
                select </span>{
                case &lt;-m.refreshTicker.C:<span class="cov0" title="0">
                        m.logger.V(2).Info("Performing scheduled cluster refresh")
                        if err := m.RefreshClusters(ctx); err != nil </span><span class="cov0" title="0">{
                                m.logger.Error(err, "Scheduled cluster refresh failed")
                        }</span>
                case &lt;-m.stopChan:<span class="cov0" title="0">
                        return</span>
                case &lt;-ctx.Done():<span class="cov0" title="0">
                        return</span>
                }
        }
}

// RefreshClusters manually refreshes cluster information
func (m *Manager) RefreshClusters(ctx context.Context) error <span class="cov0" title="0">{
        m.logger.V(2).Info("Refreshing cluster information")

        if err := m.client.DiscoverClusters(ctx); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("cluster discovery failed: %w", err)
        }</span>

        <span class="cov0" title="0">if err := m.client.DownloadKubeConfigs(ctx); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("kubeconfig download failed: %w", err)
        }</span>

        <span class="cov0" title="0">m.logger.V(2).Info("Cluster refresh completed")
        return nil</span>
}

// GetClusters returns all discovered clusters
func (m *Manager) GetClusters() map[string]*ClusterInfo <span class="cov0" title="0">{
        return m.client.GetClusters()
}</span>

// GetCluster returns information about a specific cluster
func (m *Manager) GetCluster(name string) (*ClusterInfo, error) <span class="cov0" title="0">{
        return m.client.GetCluster(name)
}</span>

// GetKubeConfigPath returns the path to a cluster's kubeconfig file
func (m *Manager) GetKubeConfigPath(clusterName string) (string, error) <span class="cov0" title="0">{
        cluster, err := m.client.GetCluster(clusterName)
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>

        <span class="cov0" title="0">if cluster.KubeConfig == "" </span><span class="cov0" title="0">{
                return "", fmt.Errorf("kubeconfig path not available for cluster %s", clusterName)
        }</span>

        <span class="cov0" title="0">return cluster.KubeConfig, nil</span>
}

// ListClusterNames returns a slice of all cluster names
func (m *Manager) ListClusterNames() []string <span class="cov0" title="0">{
        clusters := m.client.GetClusters()
        names := make([]string, 0, len(clusters))
        for name := range clusters </span><span class="cov0" title="0">{
                names = append(names, name)
        }</span>
        <span class="cov0" title="0">return names</span>
}

// GetClustersByEnvironment returns clusters filtered by environment
func (m *Manager) GetClustersByEnvironment(environment string) map[string]*ClusterInfo <span class="cov0" title="0">{
        allClusters := m.client.GetClusters()
        filtered := make(map[string]*ClusterInfo)

        for name, cluster := range allClusters </span><span class="cov0" title="0">{
                if cluster.Environment == environment </span><span class="cov0" title="0">{
                        filtered[name] = cluster
                }</span>
        }

        <span class="cov0" title="0">return filtered</span>
}

// ValidateCluster checks if a cluster exists and is accessible
func (m *Manager) ValidateCluster(clusterName string) error <span class="cov0" title="0">{
        cluster, err := m.client.GetCluster(clusterName)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Check if kubeconfig file exists
        <span class="cov0" title="0">if cluster.KubeConfig != "" </span><span class="cov0" title="0">{
                if !fileExists(cluster.KubeConfig) </span><span class="cov0" title="0">{
                        return fmt.Errorf("kubeconfig file not found: %s", cluster.KubeConfig)
                }</span>
        }

        <span class="cov0" title="0">return nil</span>
}

// IsHealthy performs a health check of the NSK integration
func (m *Manager) IsHealthy(ctx context.Context) error <span class="cov0" title="0">{
        return m.client.IsHealthy(ctx)
}</span>

// GetStatus returns the current status of the NSK manager
func (m *Manager) GetStatus() *ManagerStatus <span class="cov0" title="0">{
        m.mutex.RLock()
        defer m.mutex.RUnlock()

        clusters := m.client.GetClusters()
        status := &amp;ManagerStatus{
                Running:      m.running,
                ClusterCount: len(clusters),
                AutoRefresh:  m.config.AutoRefresh,
                LastRefresh:  time.Time{},
        }

        // Find the most recent refresh time
        for _, cluster := range clusters </span><span class="cov0" title="0">{
                if cluster.LastRefresh.After(status.LastRefresh) </span><span class="cov0" title="0">{
                        status.LastRefresh = cluster.LastRefresh
                }</span>
        }

        <span class="cov0" title="0">if m.config.AutoRefresh </span><span class="cov0" title="0">{
                status.RefreshInterval = m.config.RefreshInterval
        }</span>

        <span class="cov0" title="0">return status</span>
}

// ManagerStatus represents the current status of the NSK manager
type ManagerStatus struct {
        Running         bool      `json:"running"`
        ClusterCount    int       `json:"cluster_count"`
        AutoRefresh     bool      `json:"auto_refresh"`
        RefreshInterval string    `json:"refresh_interval,omitempty"`
        LastRefresh     time.Time `json:"last_refresh"`
}

// fileExists checks if a file exists
func fileExists(filename string) bool <span class="cov0" title="0">{
        _, err := os.Stat(filename)
        return err == nil
}</span>
</pre>
		
		<pre class="file" id="file25" style="display: none">package output

import (
        "bytes"

        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
        "k8s.io/apimachinery/pkg/runtime"
        "k8s.io/cli-runtime/pkg/printers"
        yml "sigs.k8s.io/yaml"
)

var Yaml = &amp;yaml{}

var Table = &amp;table{}

type Output interface {
        // GetName returns the name of the output format, will be used by the CLI to identify the output format.
        GetName() string
        // AsTable true if the kubernetes request should be made with the `application/json;as=Table;v=0.1` header.
        AsTable() bool
        // PrintObj prints the given object as a string.
        PrintObj(obj runtime.Unstructured) (string, error)
}

var Outputs = []Output{
        Yaml,
        Table,
}

var Names []string

func FromString(name string) Output <span class="cov0" title="0">{
        for _, output := range Outputs </span><span class="cov0" title="0">{
                if output.GetName() == name </span><span class="cov0" title="0">{
                        return output
                }</span>
        }
        <span class="cov0" title="0">return nil</span>
}

type yaml struct{}

func (p *yaml) GetName() string <span class="cov1" title="1">{
        return "yaml"
}</span>
func (p *yaml) AsTable() bool <span class="cov0" title="0">{
        return false
}</span>
func (p *yaml) PrintObj(obj runtime.Unstructured) (string, error) <span class="cov0" title="0">{
        return MarshalYaml(obj)
}</span>

type table struct{}

func (p *table) GetName() string <span class="cov1" title="1">{
        return "table"
}</span>
func (p *table) AsTable() bool <span class="cov0" title="0">{
        return true
}</span>
func (p *table) PrintObj(obj runtime.Unstructured) (string, error) <span class="cov1" title="1">{
        var objectToPrint runtime.Object = obj
        withNamespace := false
        if obj.GetObjectKind().GroupVersionKind() == metav1.SchemeGroupVersion.WithKind("Table") </span><span class="cov0" title="0">{
                t := &amp;metav1.Table{}
                if err := runtime.DefaultUnstructuredConverter.FromUnstructured(obj.UnstructuredContent(), t); err == nil </span><span class="cov0" title="0">{
                        objectToPrint = t
                        // Process the Raw object to retrieve the complete metadata (see kubectl/pkg/printers/table_printer.go)
                        for i := range t.Rows </span><span class="cov0" title="0">{
                                row := &amp;t.Rows[i]
                                if row.Object.Raw == nil || row.Object.Object != nil </span><span class="cov0" title="0">{
                                        continue</span>
                                }
                                <span class="cov0" title="0">row.Object.Object, err = runtime.Decode(unstructured.UnstructuredJSONScheme, row.Object.Raw)
                                // Print namespace if at least one row has it (object is namespaced)
                                if err == nil &amp;&amp; !withNamespace </span><span class="cov0" title="0">{
                                        switch rowObject := row.Object.Object.(type) </span>{
                                        case *unstructured.Unstructured:<span class="cov0" title="0">
                                                withNamespace = rowObject.GetNamespace() != ""</span>
                                        }
                                }
                        }
                }
        }
        <span class="cov1" title="1">buf := new(bytes.Buffer)
        // TablePrinter is mutable and not thread-safe, must create a new instance each time.
        printer := printers.NewTablePrinter(printers.PrintOptions{
                WithNamespace: withNamespace,
                WithKind:      true,
                Wide:          true,
                ShowLabels:    true,
        })
        err := printer.PrintObj(objectToPrint, buf)
        return buf.String(), err</span>
}

func MarshalYaml(v any) (string, error) <span class="cov0" title="0">{
        switch t := v.(type) </span>{
        //case unstructured.UnstructuredList:
        //        for i := range t.Items {
        //                t.Items[i].SetManagedFields(nil)
        //        }
        //        v = t.Items
        case *unstructured.UnstructuredList:<span class="cov0" title="0">
                for i := range t.Items </span><span class="cov0" title="0">{
                        t.Items[i].SetManagedFields(nil)
                }</span>
                <span class="cov0" title="0">v = t.Items</span>
        //case unstructured.Unstructured:
        //        t.SetManagedFields(nil)
        case *unstructured.Unstructured:<span class="cov0" title="0">
                t.SetManagedFields(nil)</span>
        }
        <span class="cov0" title="0">ret, err := yml.Marshal(v)
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov0" title="0">return string(ret), nil</span>
}

func init() <span class="cov1" title="1">{
        Names = make([]string, 0)
        for _, output := range Outputs </span><span class="cov10" title="2">{
                Names = append(Names, output.GetName())
        }</span>
}
</pre>
		
		</div>
	</body>
	<script>
	(function() {
		var files = document.getElementById('files');
		var visible;
		files.addEventListener('change', onChange, false);
		function select(part) {
			if (visible)
				visible.style.display = 'none';
			visible = document.getElementById(part);
			if (!visible)
				return;
			files.value = part;
			visible.style.display = 'block';
			location.hash = part;
		}
		function onChange() {
			select(files.value);
			window.scrollTo(0, 0);
		}
		if (location.hash != "") {
			select(location.hash.substr(1));
		}
		if (!visible) {
			select("file0");
		}
	})();
	</script>
</html>
